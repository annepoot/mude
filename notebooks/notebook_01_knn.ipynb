{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cc0211-9177-4a1f-8d1f-ed2b0ffee164",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://raw.githubusercontent.com/fmeer/public-files/main/TUlogo.png\" WIDTH=200 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Supervised Machine Learning for Regression - Introduction and k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from mude_tools import magicplotter\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f923ee",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A very general problem often encountered is predicting some target variable $t$ as a function of a predictor variable $x$, but we only have inaccurate measurements of our target variable. Some examples of possible problems where we can apply this kind of framework are given below:\n",
    "\n",
    "| Problem | Target variable $t$ | Predictor variable $x$ |\n",
    "| :-- | :-- | :-- |\n",
    "| Structural health monitoring of a bridge | Displacement| Location along the bridge |\n",
    "| Fatigue testing of a steel specimen | Stress | Number of loading cycles |\n",
    "| Flood control of the river Maas | Volumetric flow rate | Precipitation in the Ardennes |\n",
    "| Radioactive decay of a radon-222 sample | Number of $\\alpha$-particles emitted | Time |\n",
    "| Cooling rate of my coffee cup | Temperature | Time |\n",
    "\n",
    "To strip the problem down to its bare essentials, we will use a toy problem, where we actually know the ground truth $f(x)$. In our case, this will be a simple sine wave, of which we make 100 noisy measurements from $x=0$ to $x=2 \\pi$. Note that, in a practical setting, we would only have access to these noisy measurements and not to the true function that generated the data. Finding a good estimate of $f(x)$ based on this contaminated data is one of our main objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b15afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true function relating t to x\n",
    "def f_truth(x, freq=1, **kwargs):\n",
    "    \n",
    "    # Return a sine with a frequency of freq\n",
    "    return np.sin(x * freq)\n",
    "\n",
    "# The data generation function\n",
    "def f_data(epsilon=0.7, N=100, **kwargs):\n",
    "\n",
    "    # Apply a seed if one is given\n",
    "    if 'seed' in kwargs:\n",
    "        np.random.seed(kwargs['seed'])\n",
    "\n",
    "    # Get the minimum and maximum\n",
    "    xmin = kwargs.get('xmin', 0)\n",
    "    xmax = kwargs.get('xmax', 2*np.pi)\n",
    "    \n",
    "    # Generate N evenly spaced observation locations\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    \n",
    "    # Generate N noisy observations (1 at each location)\n",
    "    t = f_truth(x, **kwargs) + np.random.normal(0, epsilon, N)\n",
    "    \n",
    "    # Return both the locations and the observations\n",
    "    return x, t\n",
    "\n",
    "# Get the observed data\n",
    "x, t = f_data()\n",
    "\n",
    "# Plot the data and the ground truth\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(x, f_truth(x), 'k-', label=r'Ground truth $f(x)$')\n",
    "plt.plot(x, t, 'x', label=r'Noisy data $(x,t)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2adb6e",
   "metadata": {},
   "source": [
    "## k-nearest neighbors\n",
    "\n",
    "A (perhaps naive) approach to find $y(x)$ (i.e. the approximation of $f(x)$) would be to simply look at the surrounding data points and take their average to get an estimate of $f(x)$. This approach is called __k-nearest neighbors__, where $k$ refers to the number of surrounding points we are looking at.\n",
    "\n",
    "Implementing this is not trivial, but thankfully we can leverage existing implementations. We will use the `KNeighborsRegressor` function from the `sklearn.neighbors` library to fit our data and get $y(x)$ to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction locations\n",
    "# (note that these are different from the locations where we observed our data)\n",
    "x_pred = np.linspace(0, 2*np.pi, 1000)\n",
    "\n",
    "# Define a function that makes a KNN prediction at the given locations, based on the given (x,t) data\n",
    "def KNN(x, t, x_pred, k=1, **kwargs):\n",
    "    \n",
    "    # Convert x and x_pred to a column vector in order for KNeighborsRegresser to work\n",
    "    X = x.reshape(-1,1)\n",
    "    X_pred = x_pred.reshape(-1,1)\n",
    "    \n",
    "    # Train the KNN based on the given (x,t) data\n",
    "    neigh = KNeighborsRegressor(k)\n",
    "    neigh.fit(X, t)\n",
    "    \n",
    "    # Make a prediction at the locations given by x_pred\n",
    "    y = neigh.predict(X_pred)\n",
    "    \n",
    "    # Check if the regressor itself should be returned\n",
    "    if kwargs.get('return_regressor', False):\n",
    "\n",
    "        # If so, return the fitted KNN regressor\n",
    "        return neigh\n",
    "\n",
    "    else:\n",
    "\n",
    "        # If not, return the predicted values\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b45de-f5cf-443b-988e-29eda71b0320",
   "metadata": {},
   "source": [
    "We visualize the predictions with the help of the `magicplotter` object, which functions as a wrapper for `matplotlib` and enables us to reduce the amount of code contained in this notebook significantly. You do not need to understand how it works, but you can take a look at it via the course repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50691ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9,6), sharex='all', sharey='all') \n",
    "plt.suptitle(r'k-nearest neighbors regression for different values of $k$')\n",
    "\n",
    "# Plot for k=1\n",
    "magicplotter(f_data, f_truth, KNN, x_pred, k=1, ax=ax[0][0], hide_legend=True, title=r'$k={k}$')\n",
    "\n",
    "# Plot for k=10\n",
    "magicplotter(f_data, f_truth, KNN, x_pred, k=10, ax=ax[0][1], hide_legend=True, title=r'$k={k}$')\n",
    "\n",
    "# Plot for k=30\n",
    "magicplotter(f_data, f_truth, KNN, x_pred, k=30, ax=ax[1][0], hide_legend=True, title=r'$k={k}$')\n",
    "\n",
    "# Plot for k=100\n",
    "magicplotter(f_data, f_truth, KNN, x_pred, k=100, ax=ax[1][1], hide_legend=True, title=r'$k={k}$')\n",
    "\n",
    "# Add a general legend at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "handles, labels = ax[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cb5f3",
   "metadata": {},
   "source": [
    "Looking at the previous plots, a few questions might pop up:\n",
    "- For $k=1$, we see that our prediction matches the observed data exactly in all data points. Is this desirable?\n",
    "- For $k=30$, what is going on around $x=0$ and $x=2 \\pi$?\n",
    "- For $k=100$, why is our prediction constant with respect to $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346df44",
   "metadata": {},
   "source": [
    "## Varying our model parameters\n",
    "\n",
    "Clearly, some value of $k$ between 1 and 100 would give us the best predictions. Using the script below, you can generate a plot where the following variables can be adjusted:\n",
    "- $N$, the size of the training data set\n",
    "- $\\varepsilon$, the level of noise associated with the data\n",
    "- $k$, the number of neighbors over which the average is taken\n",
    "- The seed can be updated to generate new random data sets\n",
    "- The oscillation frequency of the underlying ground truth model, which controls how nonlinear the approximation should be\n",
    "- A probing location $x_0$, allowing you to see which neighbors are used to get the average response $y(x_0)$ of the kNN estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73052e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = magicplotter(f_data, f_truth, KNN, x_pred)\n",
    "plot1.add_sliders('epsilon', 'k', 'N', 'freq')\n",
    "plot1.add_buttons('truth', 'seed', 'reset')\n",
    "plot1.add_probe()\n",
    "plot1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b47a7",
   "metadata": {},
   "source": [
    "## Playing around with the plots\n",
    "\n",
    "By visual inspection, use the slider of $k$ to find its optimal value. The following questions might be interesting to ask yourself:\n",
    "- If the training size $N$ increases/decreases, how does this affect my optimal value of $k$?\n",
    "- If the frequency increases/decreases, how does this affect my optimal value of $k$?\n",
    "- If my measurements are less/more noisy, how does this affect my optimal value of $k$?\n",
    "- If I generate new data by changing the seed, how is my prediction affected for small values of $k$? What about large values of $k$?\n",
    "- So far, all observations were distributed uniformly over $x$. How would our predictions change if our observed data was more clustered?\n",
    "- If I do not know the truth, how do I figure out what my value of $k$ should be? **In the next video you will see how!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d1c7d",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "So far, we have looked at our k-nearest neighbors regressor mostly qualitatively. However, it is possible to apply a more quantitative framework to our model and find the optimal value for $k$ in a more structured way. The following lecture and accompanying notebook will discuss how this is done. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
