{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369719c6-96f2-44a1-8e4e-a95d4d7aab9c",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://raw.githubusercontent.com/fmeer/public-files/main/TUlogo.png\" WIDTH=200 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Supervised Machine Learning for Regression - Linear Basis Function Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa271d02-4ab3-4773-8846-dd7520c66ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mude_tools import magicplotter\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4d0fd-ec99-4e92-be60-69cceb55cf63",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "So far, we have been using a non-parametric model with k-nearest neighbors, meaning we needed access to the whole training dataset for each prediction. We will now focus on parametric models, namely linear models with basis functions. Parametric models are defined by a finite set of parameters calibrated in a training step. All we need for a prediction then are the parameter values. There is no longer a need to carry the whole dataset with us; the information used to make predictions is encoded in the model parameters. Once again, we will employ the simple sine function to demonstrate the concepts presented in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630454c8-740b-4eb8-8e26-bf45e45dd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true function relating t to x\n",
    "def f_truth(x, freq=1, **kwargs):\n",
    "    \n",
    "    # Return a sine with a frequency of f\n",
    "    return np.sin(x * freq)\n",
    "\n",
    "# The data generation function\n",
    "def f_data(epsilon=0.7, N=100, **kwargs):\n",
    "\n",
    "    # Apply a seed if one is given\n",
    "    if 'seed' in kwargs:\n",
    "        np.random.seed(kwargs['seed'])\n",
    "\n",
    "    # Get the minimum and maximum\n",
    "    xmin = kwargs.get('xmin', 0)\n",
    "    xmax = kwargs.get('xmax', 2*np.pi)\n",
    "    \n",
    "    # Generate N evenly spaced observation locations\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    \n",
    "    # Generate N noisy observations (1 at each location)\n",
    "    t = f_truth(x, **kwargs) + np.random.normal(0, epsilon, N)\n",
    "    \n",
    "    # Return both the locations and the observations\n",
    "    return x, t\n",
    "\n",
    "# Get the observed data\n",
    "x, t = f_data()\n",
    "\n",
    "# Plot the data and the ground truth\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.2,0.1, 0.7, 0.8])\n",
    "\n",
    "plt.plot(x, f_truth(x), 'k-', label=r'Ground truth $f(x)$')\n",
    "plt.plot(x, t, 'x', label=r'Noisy data $(x,t)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf07109-113f-4118-b5d3-9cea1a2c5ce1",
   "metadata": {},
   "source": [
    "## Linear model\n",
    "\n",
    "The key idea behind linear regression models is that they are linear in their parameters $\\mathbf{w}$. They might be linear in their inputs as well, although this does not necessarily need to case, as we will see later on in this notebook. The simplest approach is to model our target function $y(x)$ as a linear combination of the coordinates $x$:\n",
    "\n",
    "$$ y(x,\\mathbf{w}) = w_0 + w_1 x_1 $$\n",
    "\n",
    "In the one dimensional case, this is equivalent to fitting a straight line through our datapoints. The parameter $w_0$, also referred to as bias (**not to be confused with the model bias from the previous notebook**), determines the intercept, $w_1$ determines the slope. The introduction of a dummy input $x_0 = 1$ allows us to write the model in a more concise way:\n",
    "\n",
    "$$ y(x,\\mathbf{w}) = w_0 x_0 + w_1 x_1 = \\mathbf{w}^T \\mathbf{x}$$\n",
    "\n",
    "<!-- Let us define a few things we need to generate the data and fit the model first, and then see how this model performs. Once again, we employ the sine wave from the previous notebooks and generate a dataset consisting of $N$ noisy observations of the true process.\n",
    "\n",
    "$$ t = y( x, \\mathbf w ) + \\epsilon \\hspace{0.6cm} \\mathrm{with} \\hspace{0.6cm} \\epsilon \\sim \\mathcal{N} (0,\\beta^{-1}) $$ -->\n",
    "\n",
    "We will use the least squares error function from the previous notebook to fit our model, but will first show how this choice is motivated by a Maximum likelihood approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd473f0-0b5b-486f-b41b-f15c2ddd0126",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Oftentimes, it is assumed that the target $t$ is given by a deterministic function $y(\\mathbf{x}, \\mathbf{w})$ with additive Gaussian noise so that\n",
    "\n",
    "$$\n",
    "t = y(\\mathbf{x}, \\mathbf w ) + \\epsilon \\hspace{0.6cm} \\mathrm{with} \\hspace{0.6cm} \\epsilon \\sim \\mathcal{N} (0,\\beta^{-1}),\n",
    "$$\n",
    "\n",
    "with precision $\\beta$ (which is defined as $1/\\sigma^2$). Note that this assumption is only justified in case of a unimodal conditional distribution for $t$. It can have grave influence on the model accuracy and its validity therefore must be carefully assessed.\n",
    "\n",
    "As seen in the previous notebook, for the square loss function, the optimal prediction for some value $\\mathbf{x}$ is given by the conditional mean of the target variable $t$. In this case, this conditional mean is given by:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[t|\\mathbf{x}] = \\int t p(t|\\mathbf{x}) \\mathrm{d}t = y(\\mathbf{x},\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "Given a new input $\\mathbf{x}$, the distribution of $t$ that follows from our model is\n",
    "\n",
    "$$\n",
    "p (t | \\mathbf{x}, \\mathbf{w}, \\beta) = \\mathcal{N} (t | y (\\mathbf{x}, \\mathbf{w}), \\beta^{-1}).\n",
    "$$\n",
    "\n",
    "Consider now a dataset $\\mathcal{D}$ consisting of inputs $\\mathbf{X} = \\{ \\mathbf{x}_1, \\dots, \\mathbf{x}_n \\}$ and targets $\\mathbf{t} = \\{  t_1, \\dots, t_n \\}$. Assuming our datapoints are drawn independently from the same distribution (*i.i.d. assumption*), the likelihood of drawing this dataset from our model is\n",
    "\n",
    "$$p( \\mathcal{D}|\\mathbf{w}) =  \\prod_{n = 1}^N \\mathcal{N} (t_n | y (\\mathbf{x}_n, \\mathbf{w}), \\beta^{-1}), $$\n",
    "\n",
    "also referred to as the likelihood function. Taking the logarithm and expanding on the classic expression for a multivariate Gaussian distribution (see Section 2.3 of *Pattern Recognition and Machine Learning* by C.M Bishop) gives:\n",
    "\n",
    "$$ \\mathrm{ln} \\, p( \\mathcal{D}|\\mathbf{w}) = \\sum_{n = 1}^{N} \\mathrm{ln} \\, \\mathcal{N} ( t_n | \\mathbf{w}^T \\mathbf{x}_n, \\beta^{-1}) = \\frac{N}{2} \\mathrm{ln} \\beta - \\frac{N}{2}\\mathrm{ln}(2 \\pi) - \\beta \\, \\underbrace{\\frac{1}{2} \\sum_{n = 1}^{N} ( t_n - \\mathbf{w}^T \\mathbf{x}_n)^2}_{E_\\mathcal{D}}$$\n",
    "\n",
    "where we can identify our square-error loss function in the last term. Note that the first two terms are constant for a given dataset and have no influence on the parameter setting $\\bar{\\mathbf{w}}$ that maximizes the likelihood. Those optimal parameters values $\\bar{\\mathbf{w}}$ can be obtained by setting the gradient of our loss function w.r.t. $\\mathbf{w}$ to zero and solving for $\\mathbf{w}$.\n",
    "\n",
    "$$ \\nabla_{\\mathbf{w}} E_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^N \\big(t_n - \\mathbf{w}^T \\mathbf{x}_n \\big) \\mathbf{x}_n  \\stackrel{!}{=} 0 $$\n",
    "\n",
    "It is convenient to concatenate all inputs to a *design matrix* $\\mathbf{X} = [\\mathbf{x}_1^T, ..., \\mathbf{x}_N^T]^T$. Solving for $\\mathbf{w}$ gives\n",
    "\n",
    "$$ \\bar{\\mathbf{w}} = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\mathbf{X}^T \\mathbf{t} $$\n",
    "\n",
    "which is the classical expression for a least-squares solution you have by now seen many times during the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9cd083-df9a-421f-a3bf-f18003cbd516",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "Before we move on to fitting the model, we normalize our data. **This step is recommended for most machine learning techniques and is often even necessary.** A non-normalized dataset almost always leads to a numerically more challenging optimization problem. Part of model selection is to ensure that our basis functions show the desired behavior in the relevant parts of the domain. We center and rescale our data, a process referred to as standardization, to operate in the vicinity of the origin only. We use the `StandardScaler` class from the `sklearn.preprocessing` library to carry out the standardization. The standardized dataset $\\hat{\\mathcal{D}} = ( \\hat{x}, \\hat{t} )$ is obtained by subtracting the sample mean $\\mu$, and dividing by the sample standard deviation $\\sigma$ of the data:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu_x}{\\sigma_x}  \\quad \\mathrm{and} \\quad \\hat{t} = \\frac{t - \\mu_t}{\\sigma_t}.\n",
    "$$\n",
    "\n",
    "Take a look below at the standardized and unstandardized data. Note that the standardization of the target $t$ has a marginal effect, as the sine function is already centered at 0 and almost shows a standard deviation of 1. A 4 by 4 square has been added to indicate the region from $-2 \\hat{\\sigma}$ to $2 \\hat{\\sigma}$. As you can see, all input and output variables fall roughly in this interval, and this property allows for more stability when applying numerical solvers to the problem.\n",
    "\n",
    "Note that **there is no strictly correct way to shift and scale input data**. Depending on the distribution of the data, a min-max scaling or a quantile scaling might lead to a better numerical setup. The dataset's structure needs to be assessed carefully to make an informed decision on normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1776378-434a-4a3f-880b-446c9bfd77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data, instantiate scaler, and fit tranform\n",
    "np.random.seed(0)\n",
    "x, t = f_data(N=100)\n",
    "xscaler, tscaler = StandardScaler(), StandardScaler()\n",
    "x_norm, t_norm = xscaler.fit_transform(x[:,None]), tscaler.fit_transform(t[:,None])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.1,0.1, 0.8, 0.8])\n",
    "ax.plot(x,t,'x', label='unnormalized data')\n",
    "ax.plot(x_norm,t_norm,'x', label='data after normalization')\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((-2, -2), 4, 4, linewidth=1., edgecolor='k', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b38db-b848-4d89-a4bf-ed76773650da",
   "metadata": {},
   "source": [
    "With that out of the way, let us now define a few tools we need to state, solve, and visualize our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58856941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the basis functions\n",
    "def LinearBasis(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Represents a 1D linear basis.\n",
    "    \"\"\"\n",
    "    num_basis = 2  # The number of basis functions is 2 due to the dummy input\n",
    "    x = x.reshape(-1,1)\n",
    "    return np.hstack((np.ones_like(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our implementation\n",
    "print(\"Design matrix X given by:\\n\\n\", LinearBasis(np.arange(0,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8efa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction locations\n",
    "# (note that these are different from the locations where we observed our data)\n",
    "# x_pred = np.linspace(-1, 2*np.pi+1, 1000)\n",
    "\n",
    "# Define a function that makes a prediction at the given locations, based on the given (x,t) data\n",
    "def predict(x, t, x_pred, basis, normalize = True, **kwargs):\n",
    "    \n",
    "    # reshape if necessary for scalers\n",
    "    x = x[:,None] if len(x.shape)==1 else x\n",
    "    t = t[:,None] if len(t.shape)==1 else t\n",
    "    x_pred = x_pred[:,None] if len(x_pred.shape)==1 else x_pred\n",
    "    \n",
    "    # normalize data (you will see why we have to do this further below)\n",
    "    xscaler, tscaler = StandardScaler(), StandardScaler()\n",
    "    if normalize == True:\n",
    "        x_sc, t_sc = xscaler.fit_transform(x), tscaler.fit_transform(t)\n",
    "    else:\n",
    "        x_sc, t_sc = x, t\n",
    "\n",
    "    \n",
    "    # Get the variable matrix using the basis function phi\n",
    "    Phi = basis(x_sc.reshape(-1), **kwargs)\n",
    "    t_sc = t_sc.reshape(-1)\n",
    "    if normalize == True:\n",
    "        x_pred = xscaler.transform(x_pred).reshape(-1)\n",
    "    else:\n",
    "        x_pred = x_pred.reshape(-1)\n",
    "\n",
    "    \n",
    "    # Get the coefficient vector\n",
    "    w = np.linalg.solve(Phi.T @ Phi, Phi.T @ t_sc)\n",
    "    \n",
    "    # Make a prediction in the prediction locations\n",
    "    Phi_pred = basis(x_pred, **kwargs)\n",
    "    t_pred = Phi_pred @ w\n",
    "    \n",
    "    # Return the predicted values\n",
    "    if normalize == True:\n",
    "        return tscaler.inverse_transform(t_pred[:,None]).reshape(-1)\n",
    "    else:\n",
    "        return t_pred[:,None].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6abf26-c1c2-4eef-8a7e-9770b6999458",
   "metadata": {},
   "source": [
    "Note that we are **not** inverting $\\mathbf{X}^T \\mathbf{X}$, which is extremely expensive for large amounts of data (try to relate this to what you learned on weeks 2.1 and 2.2!). A more efficient way to obtain $\\mathbf{w}$ is to solve $\\mathbf{X}^T \\mathbf{X} \\mathbf{w} = \\mathbf{X}^T \\mathbf{t}$. It is important to note that this system can only be solved if $\\mathbf{X}$ has full column rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee4f17-3283-45ba-8b29-eed2f1691b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our model with linear basis funcitons and plot the results\n",
    "x_pred = np.linspace(-1, 2*np.pi+1, 1000)\n",
    "plot = magicplotter(f_data, f_truth, predict, x_pred, basis=LinearBasis, pred_label='Prediction $y(x)$', height=4.5)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab970576-5872-481b-8cb9-e00158aa2255",
   "metadata": {},
   "source": [
    "It is clear from the plot that a linear model with linear features lacks the flexibility to fit the data well. A bias-variance decomposition analysis for this model would show that it has little variance but shows a strong bias. We now consider nonlinear functions of the input $x$ as features/regressors to increase the flexibility of our linear model. A common approach is to use a set of polynomial basis functions,\n",
    "\n",
    "$$ \\phi_j(x) = x^j, $$\n",
    "\n",
    "but numerous other choices are possible. The full formulation for a model with $M$ polynomial basis functions is thus\n",
    "\n",
    "$$ y(x,\\mathbf{w}) = w_0 x^0 + w_1 x^1 + w_2 x^2 + ... + w_M x^M, $$\n",
    "\n",
    "which shows how the model is still linear w.r.t. $\\mathbf{w}$, even though it is no longer linear in the input parameters. The design matrix for this more general case reads\n",
    "\n",
    "$$ \\Phi_{ij} = \\phi_j(x_i). $$\n",
    "\n",
    "As was the case for $\\mathbf{X}$, we need to ensure that $\\boldsymbol \\Phi$ has full column rank. This is not always the case the case, for example if we have more basis functions that data points, or if our basis functions are not linearly independent.\n",
    "\n",
    "Let's implement a `PolynomialBasis` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb098b-8592-42d2-9a22-4e4c4085af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function for the polynomial basis functions:\n",
    "def PolynomialBasis(x, degree, **kwargs):  #**kwargs):\n",
    "    \"\"\"\n",
    "    A function that computes polynomial basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    x       -  The datapoints\n",
    "    degree  -  The degree of the polynomial\n",
    "    \"\"\"\n",
    "    return np.array([x ** i for i in range(degree + 1)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a44a00-0adb-42e2-98fa-8bac52777796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our implementation and visualize the polynomial basis functions\n",
    "degree = 5\n",
    "\n",
    "x_test = np.linspace(-1,1,100)\n",
    "Phi_p_test = PolynomialBasis(x_test, degree=degree)[:,1::]\n",
    "\n",
    "# Plot the data and the ground truth\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.2,0.1, 0.7, 0.8])\n",
    "\n",
    "for i, row in enumerate(Phi_p_test.transpose()):\n",
    "    plt.plot(x_test, row, label=r'$\\phi_{}(x)$'.format(i + 1))\n",
    "    \n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$\\phi(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb1dc1-c438-43e4-97df-3a4de2ee41cb",
   "metadata": {},
   "source": [
    "We obtain the linear model with nonlinear basis functions by replacing the coordinate vector $x$ with the  feature vector $\\boldsymbol{\\phi}(x)$\n",
    "\n",
    "$$ y(x,\\mathbf{w}) = \\sum_{j=0}^M w_j \\phi_j(x) = \\mathbf{w}^T \\boldsymbol{\\phi} (x).$$\n",
    "\n",
    "The solution procedure remains the same, and we can solve for $\\bar{\\mathbf{w}}$ directly\n",
    "\n",
    "$$ \\bar{\\mathbf{w}} = \\big( \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} \\big)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{t}.$$\n",
    "\n",
    "Let's take a look at the linear model with polynomial regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ebee9-d4e7-49b3-9b37-48b033cf16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9,6), sharex='all', sharey='all') \n",
    "plt.suptitle(r'generalized linear regression for polynomials of degree $p$')\n",
    "\n",
    "# Plot for degree=2\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=2, \n",
    "             ax=ax[0][0], hide_legend=True, pred_label=r'Prediction $y(x)$', title=r'$degree={degree}$')\n",
    "\n",
    "# Plot for degree=5\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=5, \n",
    "             ax=ax[0][1], hide_legend=True, title=r'$degree={degree}$')\n",
    "\n",
    "# Plot for degree=10\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=10, \n",
    "             ax=ax[1][0], hide_legend=True, title=r'$degree={degree}$')\n",
    "\n",
    "# Plot for degree=25\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=25, \n",
    "             ax=ax[1][1], hide_legend=True, title=r'$degree={degree}$')\n",
    "\n",
    "# Add a general legend at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "handles, labels = ax[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f96d8-362d-4aa9-a3b6-3c461777a564",
   "metadata": {},
   "source": [
    "That is looking much better already. However, the quality of the fit varies significantly with the degree of the polynomial basis. There seems to be an ideal model complexity for this specific problem. Try out the interactive tool below to get an idea of the interplay of the following variables:\n",
    "- $p$, the degree of the polynomial basis\n",
    "- $N$, the size of the training data set\n",
    "- $freq$, the frequency of the underlying truth\n",
    "- $\\varepsilon$, the level of noise associated with the data\n",
    "- The seed can be updated to generate new random data sets\n",
    "- The truth can be hidden to simulate a situation that is closer to a practical setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad45efb-8e4c-4eb2-969d-321c660acdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = magicplotter(f_data, f_truth, predict, x_pred, basis=PolynomialBasis, pred_label=r'Prediction $y(x)$, $p={degree}$')\n",
    "plot1.add_sliders('epsilon', 'degree', 'N', 'freq')\n",
    "plot1.add_buttons('truth', 'seed', 'reset')\n",
    "plot1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bebc7-eec8-49de-acee-bb5152b27b8d",
   "metadata": {},
   "source": [
    "A few questions that might have crossed your minds when playing with the tool:\n",
    "\n",
    "- With a small amount of data ($N \\leq 11$), what happens if we have as many data points as parameters? $(p + 1 = N)$\n",
    "- With a small amount of data ($N \\leq 11$), what happens if we have more model parameters than data? $(p + 1 > N)$\n",
    "- We only have access to data in the interval $[0,2\\pi]$. How well does our model extrapolate beyond the data range?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71668cb",
   "metadata": {},
   "source": [
    "###  Effect of Normalization on polynomial basis functions (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f4481",
   "metadata": {},
   "source": [
    "By specifying `normalize = False` in the `magicplotter` function, you deactivate the default normalization used to fit the models (implemented in the `predict` function). Depending on the dataset and the type of basis functions, normalization can have a huge impact on the goodness of the approximation.\n",
    "\n",
    "By running the cell below, you will re-train the linear models with polynomial regressors without dataset normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9,6), sharex='all', sharey='all') \n",
    "plt.suptitle(r'generalized linear regression for polynomials of degree $p$ - dataset not normalized')\n",
    "\n",
    "# Plot for degree=2\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=2, \n",
    "             normalize = False,\n",
    "             ax=ax[0][0], hide_legend=True, pred_label=r'Prediction $y(x)$', title=r'$degree={degree}$')\n",
    "\n",
    "# Plot for degree=5\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=5, \n",
    "             normalize = False,\n",
    "             ax=ax[0][1], hide_legend=True, title=r'$degree={degree}$')\n",
    "\n",
    "# Plot for degree=10\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=10, \n",
    "             normalize = False,\n",
    "             ax=ax[1][0], hide_legend=True, title=r'$degree={degree}$')\n",
    "\n",
    "# Plot for degree=25\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=PolynomialBasis, degree=25, \n",
    "             normalize = False,\n",
    "             ax=ax[1][1], hide_legend=True, title=r'$degree={degree}$')\n",
    "\n",
    "# Add a general legend at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "handles, labels = ax[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023ce01",
   "metadata": {},
   "source": [
    "If you still have time, think about the following questions:\n",
    "* Do you notice any major differences in the plots?\n",
    "* Do you think normalization improves model fitting?\n",
    "* What happens if you increase the degree of the polynomials to a very large number, say 200?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275baf1-1755-4c4b-8d6f-9fa96c8b8149",
   "metadata": {},
   "source": [
    "## Other choices of basis functions\n",
    "\n",
    "As mentioned previously, the polynomial basis is just one choice among many to define our model. Depending on the problem setting, a different set of basis functions might lead to better results. Another popular choice is the radial basis functions (also called Gaussian basis functions), given by\n",
    "\n",
    "$$ \\phi_j(x) = \\exp\\left\\{-\\frac{(x-\\mu_j)^2}{2\\ell^2}\\right\\} \\quad \\mathrm{for} \\quad j=1,\\dots,M $$\n",
    "\n",
    "where $\\phi_j$ is centered around $\\mu_j$, $l$ determines the width, and $M$ refers to the number of basis functions. Let's implement a `RadialBasisFunctions` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30400a7c-7cd1-44b5-a16e-c3b65821efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function for the RadialBasisFunctions:\n",
    "def RadialBasisFunctions(x, M_radial, l_radial, **kwargs):\n",
    "    \"\"\"\n",
    "    A function that computes radial basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X        -  The centers of the radial basis functions.\n",
    "    M_radial -  The number of basis functions\n",
    "    l_radial -  The width of each basis function\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = np.linspace(-2, 2, M_radial)\n",
    "    num_basis = mu.shape[0]\n",
    "\n",
    "    Phi = np.ndarray((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:,i] = np.exp(-.5 * (x - mu[i]) ** 2 / l_radial ** 2)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab4abd-ee85-4d29-b2e5-4e17c175a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our implementation\n",
    "l_radial = 0.5\n",
    "M_radial = 9\n",
    "\n",
    "x_test = np.linspace(-2,2,200)\n",
    "\n",
    "Phi_radial_test = RadialBasisFunctions(x_test, M_radial=M_radial, l_radial=l_radial)\n",
    "\n",
    "# Plot the data and the ground truth\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.2,0.1, 0.7, 0.8])\n",
    "\n",
    "for i, row in enumerate(Phi_radial_test.transpose()):\n",
    "    plt.plot(x_test, row, label=r'$\\phi_{}(x)$'.format(i + 1))\n",
    "    \n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$\\phi(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec8854-5eff-41c0-8be9-c7d424fcf578",
   "metadata": {},
   "source": [
    "One of the attributes of this model is the locality of its individual functions. This means data in one part of the domain will not impact predictions in other parts of the domain. Periodicity can be achieved with a Fourier basis. Wavelets are popular in signal processing since they are localized in both frequency and space. **It is up to the user to determine which basis function properties are desired for a given problem**, and this is an important part of model selection. Try to implement some of these basis functions yourself and assess how well they compare with the pre-implemented ones.\n",
    "\n",
    "Let's see how well the linear model with radial basis functions performs on the sine wave problem. Keep in mind that the lengthscale parameter corresponds to the lengthscale in the standardized space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9cd800-de32-4eb6-b1e8-541788c8050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9,6), sharex='all', sharey='all') \n",
    "plt.suptitle(r'generalized linear regression for radial basis functions with varying $M$ and $l$')\n",
    "\n",
    "# Plot for l=0.5, M=5\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=5, l_radial=0.5, \n",
    "             ax=ax[0][0], hide_legend=True, pred_label=r'Prediction $y(x)$', title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Plot for l=0.5, M=15\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=15, l_radial=0.5, \n",
    "             ax=ax[0][1], hide_legend=True, title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Plot for l=1.5, M=5\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=5, l_radial=1.5, \n",
    "             ax=ax[1][0], hide_legend=True, title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Plot for l=1.5, M=15\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=15, l_radial=1.5, \n",
    "             ax=ax[1][1], hide_legend=True, title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Add a general legend at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "handles, labels = ax[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764a91a",
   "metadata": {},
   "source": [
    "The figure above shows four different combinations of the hyperparameters (number of basis functions and length scale). The quality of the fit depends strongly on the parameter setting, but a visual inspection indicates our model can replicate the general trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce994ba0",
   "metadata": {},
   "source": [
    "###  Effect of Normalization on radial basis functions (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8434a1",
   "metadata": {},
   "source": [
    "Similar to what we did before for the polynomial basis functions, let's check whether re-training the linear models with radial basis functions without dataset normalization yields some change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e38d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9,6), sharex='all', sharey='all') \n",
    "plt.suptitle(r'generalized linear regression for radial basis functions with varying $M$ and $l$ - dataset not normalized')\n",
    "\n",
    "# Plot for l=0.5, M=5\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=5, l_radial=0.5, \n",
    "             normalize = False,\n",
    "             ax=ax[0][0], hide_legend=True, pred_label=r'Prediction $y(x)$', title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Plot for l=0.5, M=15\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=15, l_radial=0.5, \n",
    "             normalize = False,\n",
    "             ax=ax[0][1], hide_legend=True, title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Plot for l=1.5, M=5\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=5, \n",
    "             normalize = False,\n",
    "             l_radial=1.5, ax=ax[1][0], hide_legend=True, title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Plot for l=1.5, M=15\n",
    "magicplotter(f_data, f_truth, predict, x_pred, \n",
    "             basis=RadialBasisFunctions, M_radial=15, l_radial=1.5, \n",
    "             normalize = False,\n",
    "             ax=ax[1][1], hide_legend=True, title=r'$l = {l_radial}, M = {M_radial}$')\n",
    "\n",
    "# Add a general legend at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "handles, labels = ax[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c5383",
   "metadata": {},
   "source": [
    "If you still have time, think about the following questions:\n",
    "* Do you notice any major differences in the plots?\n",
    "* Do you think normalization improves model fitting in this particular case? Compare with the polynomial basis.\n",
    "* If so, why does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3810758",
   "metadata": {},
   "source": [
    "## Exercise: add a sinusoidal basis function (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c9cc7",
   "metadata": {},
   "source": [
    "The output $t$ we are trying to approximate is generated by a deterministic sine function with additive Gaussian noise:\n",
    "\n",
    "$$ t(x) = \\sin{(2\\pi f x)} + \\mathcal{N}(0,\\epsilon^{2})\\$$ \n",
    "\n",
    "where $f$ is the ordinary frequency of the sine wave and $\\epsilon$ is the standard deviation of a Gaussian distribution centered at $\\mu = 0$ (check the `f_data` function used to generate the dataset).\n",
    "\n",
    "As you may infer from your knowledge in Signal Processing, generic sinusoidal basis functions of the form $\\phi(x) = \\sin{(2\\pi f x + \\theta})$ should work particularly well to model sinusoidal periodic signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb573e",
   "metadata": {},
   "source": [
    "Your task is to fill the blanks (indicated with `???`) in the two code cells below to: \n",
    "\n",
    "* Create a single sinusoidal basis function with arguments `frequency` (i.e., $f$) and `phase` (i.e., $\\theta$);\n",
    "* Use the `magicplotter` function to check the approximation performance of different sinusoidal waves.\n",
    "\n",
    "In particular, try the following four combinations:\n",
    "\n",
    "1. `frequency = 1` and `phase = 0`\n",
    "2. `frequency = 1.1` and `phase = 0`\n",
    "3. `frequency = 2` and `phase = 0`\n",
    "4. `frequency = 1` and `phase = 5`\n",
    "\n",
    "After implementing the changes running your code, try to answer the following questions:\n",
    "\n",
    "* Why do we get the best results for `f=1`?\n",
    "* What happens as we increase the frequency?\n",
    "* What happens when we increase the phase? \n",
    "* Set `normalize = True` in all the `magicplotter` calls. What seems to be the best frequency now? Can you explain it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6455441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the basis functions\n",
    "def SinusoidalBasis(x, frequency, phase, **kwargs):\n",
    "    \"\"\"\n",
    "    Represents a Sinusoidal basis\n",
    "    \"\"\"\n",
    "    num_basis = 2  # The number of basis functions is 2 due to the dummy input\n",
    "    x = x.reshape(-1,1)\n",
    "    Phi = ???\n",
    "    return np.hstack((np.ones_like(x), Phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f33a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9,6), sharex='all', sharey='all') \n",
    "plt.suptitle(r'generalized linear regression for radial basis functions with varying $M$ and $l$')\n",
    "\n",
    "# Plot for f=0.5, phase=5\n",
    "???\n",
    "\n",
    "# Plot for f=0.5, phase=15\n",
    "???\n",
    "\n",
    "# Plot for f=1.5, phase=5\n",
    "???\n",
    "\n",
    "# Plot for f=1.5, phase=15\n",
    "???\n",
    "\n",
    "\n",
    "# Add a general legend at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "handles, labels = ax[0][0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44517eb-601a-41fb-ad5a-4ea272357d76",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "This notebook introduced generalized linear models with arbitrarily high flexibility. We have seen that increased flexibility is not always good if we perform a simple least-squares analysis. We know from the previous notebook that we can introduce a validation set to prevent our model from overfitting; however, removing features is not always trivial. The following lecture and its accompanying notebook will introduce you to ridge regression, an elegant method for controlling the model complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
