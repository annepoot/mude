{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abfe8e4-c068-411f-85f4-c342a1ccd4e3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://raw.githubusercontent.com/fmeer/public-files/main/TUlogo.png\" WIDTH=200 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Supervised Machine Learning for Regression - Regularization and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa271d02-4ab3-4773-8846-dd7520c66ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mude_tools import magicplotter\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4d0fd-ec99-4e92-be60-69cceb55cf63",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As the previous notebook shows, the least squares approach can lead to severe over-fitting if complex models are trained on small datasets. Our strategy of limiting the number of basis functions a priori to counter overfitting puts us at risk of missing critical trends in the data. Another way to control model flexibility is by introducing a regularization term. This additional term essentially puts a penalty on the weights and prevents them from taking too large values unless supported by the data. The concept of regularized least squares will be demonstrated on the usual sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630454c8-740b-4eb8-8e26-bf45e45dd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# The true function relating t to x\n",
    "def f_truth(x, freq=1, **kwargs):\n",
    "    \n",
    "    # Return a sine with a frequency of f\n",
    "    return np.sin(x * freq)\n",
    "\n",
    "# The data generation function\n",
    "def f_data(epsilon=0.5, N=20, **kwargs):\n",
    "\n",
    "    # Apply a seed if one is given\n",
    "    if 'seed' in kwargs:\n",
    "        np.random.seed(kwargs['seed'])\n",
    "\n",
    "    # Get the minimum and maximum\n",
    "    xmin = kwargs.get('xmin', 0)\n",
    "    xmax = kwargs.get('xmax', 2*np.pi)\n",
    "    \n",
    "    # Generate N evenly spaced observation locations\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    \n",
    "    # Generate N noisy observations (1 at each location)\n",
    "    t = f_truth(x, **kwargs) + np.random.normal(0, epsilon, N)\n",
    "    \n",
    "    # Return both the locations and the observations\n",
    "    return x, t\n",
    "\n",
    "# Get the observed data\n",
    "x, t = f_data()\n",
    "x_pred = np.linspace(0, 2*np.pi, 1000)\n",
    "\n",
    "# Plot the data and the ground truth\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.2,0.1, 0.7, 0.8])\n",
    "plt.plot(x_pred, f_truth(x_pred), 'k-', label=r'Ground truth $f(x)$')\n",
    "plt.plot(x, t, 'x', label=r'Noisy data $(x,t)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "# plt.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf07109-113f-4118-b5d3-9cea1a2c5ce1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regularized least squares\n",
    "\n",
    "We extend the data-dependent error $E_D(\\bf{w})$ with the regularization term $E_W(\\bf{w})$:\n",
    "\n",
    "$$\n",
    "E (\\mathbf{w}) = E_D (\\mathbf{w}) + \\lambda E_W (\\mathbf{w})\n",
    "$$\n",
    "\n",
    "with regularization parameter $\\lambda$ that controls the relative importance of two terms comprising the error function. A common choice for the regularizer is the sum-of-squares:\n",
    "\n",
    "$$\n",
    "E_W(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "The resulting model is known as L2 regularization, ridge regression, or weight decay. The total error therefore becomes\n",
    "\n",
    "$$\n",
    "E (\\mathbf{w}) = \\frac{1}{2} \\sum_{n = 1}^{N} \\left( t_n - \\mathbf{w}^T \\boldsymbol{\\phi} (x_n) \\right)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "<!-- Taking its gradient gives\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} E_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^N \\big(t_n - \\mathbf{w}^T \\mathbf{x}_n \\big) \\mathbf{x}_n\n",
    "$$ -->\n",
    "\n",
    "A useful property of the L2 regularization is that the error functions is still a quadratic function of $\\mathbf{w}$, allowing for a closed form solution for its minimizer. Setting the gradient of the regularized error function with respect to $\\mathbf{w}$ to $0$ gives\n",
    "\n",
    "$$\n",
    "\\bar{\\mathbf{w}} = \\big( \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{I} \\big)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{t}.\n",
    "$$\n",
    "\n",
    "<!-- Keeping in mind that $\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi}$ is a positive semi-definite matrix, the eigenvalues of the inverse of $\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{I}$ will shrink with an inceasing $\\lambda$, therefore pulling the weights towards $0$ compared to the unregularized solution. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd473f0-0b5b-486f-b41b-f15c2ddd0126",
   "metadata": {},
   "source": [
    "## Determining the regularization parameter\n",
    "\n",
    "The `predict` function in the code below has been adapted to include this regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc9bd3-8065-469c-a5eb-1cf4d62ac05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data, instantiate scaler, and fit tranform\n",
    "np.random.seed(0)\n",
    "x, t = f_data(N=100)\n",
    "xscaler, tscaler = StandardScaler(), StandardScaler()\n",
    "x_norm, t_norm = xscaler.fit_transform(x[:,None]), tscaler.fit_transform(t[:,None])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.1,0.1, 0.8, 0.8])\n",
    "ax.plot(x,t,'x', label='unnormalized data')\n",
    "ax.plot(x_norm,t_norm,'x', label='data after normalization')\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((-2, -2), 4, 4, linewidth=1., edgecolor='k', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb739431-1349-43a8-b9e6-7479939dee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function for the RadialBasisFunctions:\n",
    "def RadialBasisFunctions(x, M_radial, l_radial, **kwargs):\n",
    "    \"\"\"\n",
    "    A function that computes radial basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X        -  The centers of the radial basis functions.\n",
    "    M_radial -  The number of basis functions\n",
    "    l_radial -  The width of each basis function\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = np.linspace(-2, 2, M_radial)\n",
    "    num_basis = mu.shape[0]\n",
    "\n",
    "    Phi = np.ndarray((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:,i] = np.exp(-.5 * (x - mu[i]) ** 2 / l_radial ** 2)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298142e-f19e-493b-a3dd-64291451aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that makes a prediction at the given locations, based on the given (x,t) data\n",
    "def predict(x, t, x_pred, basis, lam=None, **kwargs):\n",
    "    \n",
    "    # reshape if necessary for scalers\n",
    "    x = x[:,None] if len(x.shape)==1 else x\n",
    "    t = t[:,None] if len(t.shape)==1 else t\n",
    "    x_pred = x_pred[:,None] if len(x_pred.shape)==1 else x_pred\n",
    "    \n",
    "    # normalize data\n",
    "    xscaler, tscaler = StandardScaler(), StandardScaler()\n",
    "    x_sc, t_sc = xscaler.fit_transform(x), tscaler.fit_transform(t)\n",
    "    \n",
    "    # if 'M_radial' in kwargs:\n",
    "    #     Phi = basis (x_sc.reshape(-1), kwargs['M_radial'], kwargs['l_radial'], **kwargs)\n",
    "    # else:\n",
    "    Phi = basis(x_sc.reshape(-1), **kwargs)\n",
    "    \n",
    "    # Get the variable matrix using the basis function phi\n",
    "    t_sc = t_sc.reshape(-1)\n",
    "    x_pred = xscaler.transform(x_pred).reshape(-1)\n",
    "    \n",
    "    # Get identity matrix, set first entry to 0 to neglect bias in regularizaiton\n",
    "    I = np.identity(Phi.shape[1])\n",
    "    I[0,0] = 0.0\n",
    "    \n",
    "    # Get the coefficient vector\n",
    "    if lam is None:\n",
    "        w = np.linalg.solve( Phi.T @ Phi , Phi.T @ t_sc )\n",
    "    else:\n",
    "        w = np.linalg.solve( Phi.T @ Phi + lam * I, Phi.T @ t_sc )\n",
    "    \n",
    "    # Make a prediction in the prediction locations\n",
    "    Phi_pred = basis(x_pred, **kwargs)\n",
    "    t_pred = Phi_pred @ w\n",
    "    \n",
    "    # Return the predicted values\n",
    "    return tscaler.inverse_transform(t_pred[:,None]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e445859-ae75-4051-b1eb-993c8cc74f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's try out our ridge regression model\n",
    "x_plot = np.linspace(0, 2*np.pi, 1000)[:,None]\n",
    "\n",
    "# set seed and generate data\n",
    "np.random.seed(0)\n",
    "M_radial = 10\n",
    "l_radial = 0.5\n",
    "x_train, t_train = f_data(N=15)\n",
    "\n",
    "# Let's take a look at our regularized solution\n",
    "plot = magicplotter(f_data, f_truth, predict, x_plot, x_train, t_train,\n",
    "                    basis=RadialBasisFunctions, M_radial=M_radial, l_radial=l_radial, lam=1e-12,\n",
    "                    pred_label='Prediction $y(x)$', height=4.5)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3889a67-6c7c-43ea-adae-5e29f6dce08f",
   "metadata": {},
   "source": [
    "You can play around with the number of radial basis functions and the regularization parameter. Try to answer the following questions:\n",
    "\n",
    "- What happens for a low number of RBF, and what happens for a number thereof?\n",
    "\n",
    "- How does this affect the value of $\\lambda$ that yields the (visually) best fit?\n",
    "\n",
    "- How does a larger dataset affect the optimal regularization parameter $\\bar{\\lambda}$?\n",
    "\n",
    "\n",
    "The pressing question is, of course: **how do we determine $\\lambda$?** \n",
    "\n",
    "The problem of controlling model complexity has been shifted from choosing a suitable set of basis functions to determining the regularization parameter $\\lambda$. **Optimizing for both $\\mathbf{w}$ and $\\lambda$ over the training dataset will always favor flexibility, leading to the trivial solution $\\lambda = 0$** and, therefore, to the unregularized least squares problem. Instead, similar to what we did in notebook 2, the data should be partitioned into a training set and a validation set. The training set is used to determine the parameters $\\mathbf{w}$, and the validation set to optimize the model complexity through $\\lambda$. \n",
    "\n",
    "In the block below we use a pragmatic approach and sample another dense validation set from our ground truth instead of splitting the original training dataset. We then loop over several values of $\\lambda$ until we find the one corresponding to the minimum validation error. This is then our selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ad99c-bf44-4a14-a489-b3876d1ba867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed and generate data\n",
    "np.random.seed(4)\n",
    "x_train, t_train = f_data(N=15)\n",
    "x_val, t_val = f_data(N=500)\n",
    "\n",
    "# set up some vectores and params for the training\n",
    "lams    = np.logspace(-10,20,100,base=np.exp(1))\n",
    "mse_val = np.zeros_like(lams)\n",
    "x_plot  = np.linspace(0, 2*np.pi, 1000)[:,None]\n",
    "\n",
    "M_radial = 10\n",
    "l_radial = 0.5\n",
    "\n",
    "# loop over all lambdas \n",
    "for i, lam in enumerate(lams):\n",
    "    t_pred = predict(x_train, t_train, x_val, RadialBasisFunctions, M_radial=M_radial, l_radial=l_radial, lam=lam)\n",
    "    mse_val[i] = sum((t_val - t_pred)**2) / len(x_val)\n",
    "\n",
    "# find lambda with minimum mse\n",
    "loc = np.argmin(mse_val)\n",
    "lam_min = lams[loc]\n",
    "mse_min = mse_val[loc]\n",
    "print(\"Best fit for lambda = {:e}  with MSE = {:4f}\".format(lam_min, mse_min))\n",
    "    \n",
    "# plot mse over ln(lambda)\n",
    "fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "ax.set_position([0.2,0.1, 0.7, 0.8])\n",
    "# fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(lams, mse_val)\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "ax.set_ylabel(r'MSE')\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaabd71-fd9f-4de8-97ae-fc09e5173f37",
   "metadata": {},
   "source": [
    "We then take a look at how the optimally regularized model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee4f17-3283-45ba-8b29-eed2f1691b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our regularized solution\n",
    "plot = magicplotter(f_data, f_truth, predict, x_plot, x_train, t_train,\n",
    "                    basis=RadialBasisFunctions, M_radial=M_radial, l_radial=l_radial,  lam=lam_min,\n",
    "                    pred_label='Prediction $y(x)$',  height=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b9594-4797-4db4-87a2-a47e3c17ccf3",
   "metadata": {},
   "source": [
    "That looks much better than the solution of the unregularized problem, which you can obtain by removing $\\lambda$ from arguments when calling the `predict` function. Luckily, this problem has a unique solution $\\bar{\\lambda}$. Can you explain why the error first decreases but keeps growing again at some point? Do you always expect this non-monotonic characteristic when looking at the **training** error as a function of the regularization parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb1dc1-c438-43e4-97df-3a4de2ee41cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "For now, we used the same dataset at once. In some situations, it might be beneficial or necessary to look at only a part of the dataset, e.g., when\n",
    "\n",
    "- $N$ is too large and computing $( \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} )^{-1}$ becomes prohibitively expensive\n",
    "- the model is nonlinear in $\\mathbf{w}$, and $\\mathbf{w}_\\mathrm{ML}$ does not have a closed-form solution\n",
    "- the dataset is arriving sequentially (e.g., in real-time from a sensor)\n",
    "\n",
    "Instead of solving for $\\mathbf{w}$ directly, we could employ an iterative optimization strategy. Let's first take a look at the data part of the error function, and its gradient with respect to $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "E_D = \\frac{1}{2N} \\sum_{n = 1}^{N} \\left( t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\right)^2 \\quad \\mathrm{with \\, gradient} \\quad \\nabla E_D = - \\frac{1}{N} \\sum_{n=1}^N \\big(t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\big) \\boldsymbol{\\phi}_n.\n",
    "$$\n",
    "\n",
    "The standard formulation does not include the division by the dataset size, the stepsize is purely regulated through the learning rate. However, normalizing the gradient with $N$ makes the influence of the learning rate more consistent when considering different datasets. With a standard gradient descent algorithm, the update rule for the weights is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^{(\\tau)} - \\eta \\nabla E_D\n",
    "$$\n",
    "\n",
    "with a fixed *learning rate* $\\eta$. The costs for the gradient computations are independent of the dataset size $N$, when only considering subset $\\mathcal{B}$ of our dataset with $N_{\\mathcal{B}}$ data points. If we pick a random subset $\\mathcal{B}$ for each iteration of the optimization scheme, we have derived the *stochastic gradient descent* algorithm. Together with its numerous variants, this algorithm forms the backbone of many machine learning techniques. Most deep learning libraries, such as `TensorFlow` or `PyTorch` offer implementations of these algorithms.\n",
    "\n",
    "We looked at the unregularized model to introduce SGD, but the extension to the regularized model is straightforward. Remember, in this case, the objective function is given by\n",
    "\n",
    "$$\n",
    "E (\\mathbf{w}) = \\frac{1}{2N} \\sum_{n = 1}^{N} \\left( t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\right)^2 + \\frac{\\lambda}{2N} \\mathbf{w}^T \\mathbf{w},\n",
    "$$\n",
    "\n",
    "and its gradient with respect to $\\mathbf{w}$ reads\n",
    "\n",
    "$$\n",
    "\\nabla E = \\frac{1}{N} \\left( - \\sum_{n=1}^N \\big(t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\big) \\boldsymbol{\\phi}_n + \\lambda \\, \\mathbf{w} \\right).\n",
    "$$\n",
    "\n",
    "When looking at the expresssion in the outer bracket, it becomes clear there are **two competing terms**: the first term pulls the weights towards the data, and the second term pulls them towards zero. Looking at the gradient, you can also see why ridge regression often is referred to as weight decay. The larger the weights become, the stronger the regularization term pulls them towards zero. If the data would not support a value for a certain weight, its presence in the gradient will lead to the weight decaying at a rate proportional to its magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c500a2-376a-448e-b67f-40c1e86aa958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the gradient of the cost function\n",
    "def get_gradient(x, t, w, basis, lam=0., **kwargs):\n",
    "    \n",
    "    # Get the variable matrix using the basis function phi\n",
    "    Phi = basis(x.reshape(-1), **kwargs)\n",
    "    t = t.reshape(-1)\n",
    "    \n",
    "    return (- ( t - w @ Phi.T ) @ Phi + lam * w ) / len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbcf4e-ccdd-4c53-8039-712b01ee2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "N, N_train = 500, 20\n",
    "M_radial = 50                       # number of radial basis functions  \n",
    "l_radial = 0.20                     # lenghtscale of radial basis functions\n",
    "eta = 0.01                          # learning rate\n",
    "epochs = 1000                       # number of epochs\n",
    "N_batch = 5                         # size of the minibatch B\n",
    "lam = np.exp(0.05)                  # Regularization parameter\n",
    "\n",
    "# select where to plot solution\n",
    "epoch_plot_marks = np.logspace(np.log10(5),np.log10(epochs), 10, dtype=int)\n",
    "\n",
    "# randomly init weights\n",
    "np.random.seed(6)\n",
    "w = (np.random.rand(M_radial) - 0.5) / M_radial\n",
    "w_reg = w.copy()\n",
    "\n",
    "# get colormap\n",
    "rgb = cm.get_cmap('summer')(np.linspace(0,1,10))[:, :3]\n",
    "\n",
    "# generate, scale, and partition data\n",
    "x, t = f_data(N= N + N_train)\n",
    "x_plot = np.linspace(0,2*np.pi,100)\n",
    "xscaler, tscaler = StandardScaler(), StandardScaler()\n",
    "x_sc, t_sc = xscaler.fit_transform(x[:,None]), tscaler.fit_transform(t[:,None])\n",
    "x_train, x_val, t_train, t_val = train_test_split(x_sc, t_sc, train_size=N_train)\n",
    "x_train, x_val, t_train, t_val = x_train.reshape(-1), x_val.reshape(-1), t_train.reshape(-1), t_val.reshape(-1)\n",
    "\n",
    "# get features\n",
    "Phi_plot  = RadialBasisFunctions(xscaler.transform(x_plot[:,None]).reshape(-1), M_radial=M_radial, l_radial=l_radial)\n",
    "Phi_train = RadialBasisFunctions(x_train, M_radial=M_radial, l_radial=l_radial)\n",
    "Phi_val   = RadialBasisFunctions(x_val, M_radial=M_radial, l_radial=l_radial)\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(2, 2, figsize=(13,8), sharey='row')\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "ax[0,0].plot(xscaler.inverse_transform(x_train[:,None]), tscaler.inverse_transform(t_train[:,None]), 'x')\n",
    "ax[0,1].plot(xscaler.inverse_transform(x_train[:,None]), tscaler.inverse_transform(t_train[:,None]), 'x')\n",
    "ax[0,0].plot(x_plot, f_truth(x_plot), 'k-'), ax[0,1].plot(x_plot, f_truth(x_plot), 'k-')\n",
    "ax[0,0].set_xlabel('x'), ax[0,1].set_xlabel('x'), ax[0,0].set_ylabel('t')\n",
    "ax[1,0].set_xlabel('epochs'), ax[1,1].set_xlabel('epochs'), ax[1,0].set_ylabel('RMSE')\n",
    "ax[0,0].set_ylim(-2,2), ax[0,1].set_ylim(-2,2)\n",
    "ax[0,0].set_title(\"Unregularized\"), ax[0,1].set_title(\"Regularized\")\n",
    "\n",
    "# init rmse\n",
    "rmse_train, rmse_train_reg = np.zeros(epochs), np.zeros(epochs)\n",
    "rmse_val, rmse_val_reg = np.zeros(epochs), np.zeros(epochs)\n",
    "\n",
    "# loop over epochs\n",
    "for i in range(epochs):\n",
    "    perm = np.random.permutation(N_train)\n",
    "    \n",
    "    # loop over batches\n",
    "    for j in range(int(N_train/N_batch)):\n",
    "        x_batch  = x_train[perm[j*N_batch:(j+1)*N_batch]]\n",
    "        t_batch  = t_train[perm[j*N_batch:(j+1)*N_batch]]\n",
    "        dEdw     = get_gradient(x_batch, t_batch, w, RadialBasisFunctions, M_radial=M_radial, l_radial=l_radial)\n",
    "        dEdw_reg = get_gradient(x_batch, t_batch, w_reg, RadialBasisFunctions, lam=lam, M_radial=M_radial, l_radial=l_radial)\n",
    "        w        = w - eta * dEdw\n",
    "        w_reg    = w_reg - eta * dEdw_reg\n",
    "        \n",
    "    # get predictions\n",
    "    t_pred_train     = w     @ Phi_train.T\n",
    "    t_pred_train_reg = w_reg @ Phi_train.T\n",
    "    t_pred_val       = w     @ Phi_val.T\n",
    "    t_pred_val_reg   = w_reg @ Phi_val.T\n",
    "    \n",
    "    # compute rmse\n",
    "    rmse_train[i]     = np.sqrt(sum((tscaler.inverse_transform(t_train[:,None])\n",
    "                                     - tscaler.inverse_transform(t_pred_train[:,None]))**2) / len(t_train))\n",
    "    rmse_train_reg[i] = np.sqrt(sum((tscaler.inverse_transform(t_train[:,None])\n",
    "                                     - tscaler.inverse_transform(t_pred_train_reg[:,None]))**2) / len(t_train))\n",
    "    rmse_val[i]       = np.sqrt(sum((tscaler.inverse_transform(t_val[:,None])\n",
    "                                     - tscaler.inverse_transform(t_pred_val[:,None]))**2) / len(t_val))\n",
    "    rmse_val_reg[i]   = np.sqrt(sum((tscaler.inverse_transform(t_val[:,None])\n",
    "                                     - tscaler.inverse_transform(t_pred_val_reg[:,None]))**2) / len(t_val))\n",
    "    \n",
    "    # plot some of the iterations to see progression of our model\n",
    "    if (i+1) in epoch_plot_marks:\n",
    "        t_plot     = w     @ Phi_plot.T\n",
    "        t_plot_reg = w_reg @ Phi_plot.T\n",
    "        ax[0,0].plot(x_plot, tscaler.inverse_transform(t_plot[:,None]).reshape(-1),\n",
    "                     color=rgb[int(i/epoch_plot),:], label=r'{}'.format(i+1))\n",
    "        ax[0,1].plot(x_plot, tscaler.inverse_transform(t_plot_reg[:,None]).reshape(-1),\n",
    "                     color=rgb[int(i/epoch_plot),:], label=r'{}'.format(i+1))\n",
    "\n",
    "# put legend\n",
    "ax[0,0].legend(loc='lower left', ncol=2)\n",
    "ax[0,0].get_legend().set_title(r'epochs')\n",
    "\n",
    "# plot rmse\n",
    "ax[1,0].semilogx(np.arange(1,epochs+1), rmse_train, label=r'$RMSE_{train}$')\n",
    "ax[1,0].semilogx(np.arange(1,epochs+1), rmse_val, label=r'$RMSE_{val}$')\n",
    "\n",
    "ax[1,1].semilogx(np.arange(1,epochs+1), rmse_train_reg, label=r'$RMSE_{train}$')\n",
    "ax[1,1].semilogx(np.arange(1,epochs+1), rmse_val_reg, label=r'$RMSE_{val}$')\n",
    "ax[1,0].legend(), ax[1,1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e9d1f-b4f8-45c7-9b48-7ce37ddbe590",
   "metadata": {},
   "source": [
    "You can see in the top figure that our predictions seem to converge towards a particular shape. The remaining discrepancy between our final model and the true function $f$ is due to our general model bias, and the particular dataset we drew. What happens if you increase the number of basis functions? Can you explain why we do not approach the target function?\n",
    "\n",
    "You can see that our validation error increases at some point, indicating that overfitting might occur. Again, note how the training error cannot feel that, and just decreases monotonically. SGD with minibatches already has a slight regularizing effect. Other remedies include the L2-regularization technique discussed discusses previously, early stopping, or collecting more data. Try to see how the RMSE for the training and validation set will behave for larger datasets by adapting the training dataset size `N_train`.\n",
    "\n",
    "Finally, it should be noted that the step size of the SGD must be chosen carefully. Try out for yourself what happens when you choose very small or large stepsizes by adapting the learning rate `eta`. Even though this optimization problem is well-defined and has a global minimum, SGD is not guaranteed to converge to it. Luckily, the most popular variants such as [AdaGrad][1], [RMSProp][2], and [Adam][3] feature some form of adaptive stepsize control, improving convergence rate and robustness. One usually starts with a larger stepsize to approach the minimum quickly. After that, the stepsize is reduced continuously to reliably uncover the exact location of the extremum.\n",
    "\n",
    "[1]: https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
    "[2]: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "[3]: https://arxiv.org/abs/1412.6980"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "10"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
