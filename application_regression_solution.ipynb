{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d585d2-6e7a-4a30-b3a6-0921ae399e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from plotting import plotly_plot, format_colorbar_plot, format_colorbar_pca_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7464e1-b028-49e2-a515-b29ced4922f5",
   "metadata": {},
   "source": [
    "## General remark\n",
    "\n",
    "**This notebook contains a lot of code, especially for plotting. You do not need to understand all of it, try to focus on the parts that are hightlighted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb1954-141e-4b94-9d3d-28126aafd13f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you should apply the techniques you learned for regression in a more realistic problem setting. We have a collection of bridges modeled as 2D beams that all feature one defect. Our goal is to train a model to learn the location of this defect as a function of displacement measurements. Since sensors are expensive, we can only place them in five locations on the bridges. The assignment includes the following tasks:\n",
    "\n",
    "- Select five locations for the sensors based on a visual inspection of the displacement field \n",
    "- Pre-process the available data to use it in a neural network\n",
    "- Train a neural network to learn a mapping from the displacement measurements to the defect location, comment on the choice of hyperparameters (number of hidden layers, nodes per layer, ...)\n",
    "- Visualise your results and evaluate the accuracy of your network\n",
    "- Implement an alternative based on PCA where we have sensor data at every location\n",
    "\n",
    "Let's take a look at the dataset first. It is a CSV file, and a convenient way to read and manipulate this file type is via the `Dataframe` of the `pandas` library. Printing a few lines of the dataset before performing the analysis is good practice. We load the dataset into a `Dataframe` from the `pandas` library and print a few rows from the top and bottom. The dataset consists of a collection of displacement fields of the bridges. We have a total of 1000 bridges, as can be seen from the tail of the data frame `df.tail()`, and 712 locations in which the displacements have been measured, as can be seen from the tail of a sample `bar_0.tail()`, which is just a single sample we took from the dataset. Note that the location is uniform for a specific sample owing to the dataset's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13133063-f697-44a3-be7b-be8f1c95c4d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![overview beam structure](img/beam_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe7b1c-eff1-40aa-a190-f0b04f02f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regression-data_realistic.csv')\n",
    "bar_0 = df[df['sample'] == 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5334850-43fe-4f9c-b0f6-9439c930d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecae500-dce6-442a-8dd3-87e493137355",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_0.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cb1a6-a0ef-42f6-84fe-7ef24c99bf60",
   "metadata": {},
   "source": [
    "## Data visualization and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00df669-8188-4d24-aadd-96d3fb8ab23c",
   "metadata": {},
   "source": [
    "Your first task is to select measurement locations. The following cell plots the displacement in the x- or y-direction or the magnitude over the beam's domain. You can select the three components via the buttons on the topside. In addition, you can see all the available measurement locations. You can hover over the plot, which will display the data frame's corresponding node ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a34a5c-17de-4156-bf65-82d63d9e3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e267d18-7f85-41f7-b356-7a6f86ec5462",
   "metadata": {},
   "source": [
    "Select five measurement locations that you expect to be informative. Plug them into the predefined list `measure_locs`. Remember that we only have a budget of five locations, make sure to not exceed this threshold to secure a spot on the leaderboard (more to that later). The remaining code in this cell collects the displacements from all of our beams at the selected nodes and the defect location. These quantities are stored in the arrays `measurements` and `defect_locs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8385552-fb55-4633-a86f-1be8f0970c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define measurement locations, get corresponding coord\n",
    "measure_locs = [481, 461, 312, 41, 66]\n",
    "measure_coords = np.array([bar_0[bar_0['node'] == loc][['x','y']].to_numpy() for loc in measure_locs]).squeeze(1)\n",
    "\n",
    "# double check the measurement locations\n",
    "print(measure_coords)\n",
    "                             \n",
    "# read measurement from all samples in dataframe\n",
    "measurements = np.empty((df['sample'].max()+1,0))\n",
    "\n",
    "# loop through measurement locations and collect measuments from all samples\n",
    "for loc in measure_locs:\n",
    "    dx = df[df['node'] == loc]['dx'].to_numpy()\n",
    "    dy = df[df['node'] == loc]['dy'].to_numpy()\n",
    "    measurements = np.append(measurements, np.vstack((dx, dy)).transpose(),axis=1)\n",
    "\n",
    "# get defect locations\n",
    "defect_locs = df[df['node']==0]['location'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc9f7f-bffc-4b12-9ad4-9ee0ae1d60f0",
   "metadata": {},
   "source": [
    "Let's plot the defect location as a function of the displacement measurements to get a feel for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601760f2-2ef4-4444-b7bd-cbbc2693f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few hyperplanes of the dataset\n",
    "# ======================================== ignore code ==============================================================\n",
    "fig, ax = plt.subplots(5,2, figsize=(10,20))\n",
    "[ax.flat[i].scatter(measurements[:,i], defect_locs, s=5) for i in range(len(ax.flat))]\n",
    "[ax.flat[i].ticklabel_format(style='sci', axis='x', scilimits=(0,0)) for i in range(len(ax.flat))]\n",
    "[ax[0,i].set_title(title) for i,title in enumerate([r'$u_x$',r'$u_y$'])]\n",
    "[ax[i,0].text(-0.4, 0.46, r'node {}'.format(i+1), transform=ax[i,0].transAxes, fontsize=12) for i in range(ax.shape[0])]\n",
    "[ax[i,0].set_ylabel(r'$x_{defect}$') for i in range(ax.shape[0])]\n",
    "ticks = [np.linspace(np.min(measurements[:,i]), np.max(measurements[:,i]), 4) for i in range(measurements.shape[1])]\n",
    "[axs.set_xticks(tick) for axs, tick in zip(ax.flat, ticks)]\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6ff04-99ed-415e-b2e5-f4ef01790711",
   "metadata": {},
   "source": [
    "We can see that most measurements do not have a unique mapping to the defect location, suggesting we need multiple features to distinguish between the deformation states.\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "Change the measurement locations (2 code cells above this text) and study the influence on the plots. Finally, pick 5 sensors you believe will lead to the best results.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac55d5-81d1-4040-8601-a63b3d62d589",
   "metadata": {},
   "source": [
    "Let's take a look at a 2D scatterplot of our data. Note that this is a projection of the data on this particular 2D subspace of the input space. The color bar indicates the defect location of a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387855e1-ba3f-472f-9961-601e4ff81d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select components to plot\n",
    "#   remeber that we flattened the measurement array, meaning that\n",
    "#   even indices correspond to x-measurements, uneven incdices to y-measurements.\n",
    "#   the tuple (3,7) threfore corresponds to u_y in node 2, and u_y in node 4\n",
    "#   you can also look at the plot to see which component you are inspecting\n",
    "\n",
    "idcs = (3,7)\n",
    "skip = 5\n",
    "\n",
    "# make figure and plot data\n",
    "# ======================================== ignore code ==============================================================\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plot1 = ax.scatter(measurements[::skip,idcs[0]], measurements[::skip,idcs[1]], c=defect_locs[::skip], s=39)\n",
    "format_colorbar_plot(fig, ax, plot1, idcs)\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9912a-51ce-45b4-8c8e-a71bde49eae1",
   "metadata": {},
   "source": [
    "This should look more promising; the defect location seems to be an injective function when considering multiple measurements.\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "Select different indices to be plotted. Compare choosing only x components with choosing y components.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1a5e7-bd37-44fb-a69d-53b674e6129f",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c07e5d-d0f4-462d-b486-99cb71f7729f",
   "metadata": {},
   "source": [
    "To train a model using this data, we need to split it in a training, validation and test set. The training set is used to train the model, the validation set is used to fit the hyperparameters of the NN, and the test set is used to assess the predictive capabilities of the resulting model. The latter should not be used for model selection or training. We need truly unseen data to properly evaluate the performance of our final model.\n",
    "\n",
    "<!-- <div style=\"background-color:rgba(0, 0, 0, 0.0470588); ; vertical-align: middle; padding:10px 20px;\"> -->\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:10px 20px;\">\n",
    "<p>\n",
    "    <b>Task:</b>\n",
    "\n",
    "Complete the code below to implement a function that creates a random train, validation & test set.\n",
    "- Apply a random permutation to the data. Make sure X & y are permutated in the same manner!\n",
    "- Compute the size of the train, validation & test split\n",
    "- Select the permutated data splits\n",
    "- Select sensible fraction sizes for the validation and test set\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c9915-c6ec-47bd-86eb-7936a8c202f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split dataset into training, validation, and test set\n",
    "def train_test_val_split(X, y, val_size, test_size, seed=0):\n",
    "    \"\"\"\n",
    "    X = [N x features]\n",
    "    y = [N x outputs]\n",
    "    val_size = fraction of the full dataset becoming the validation set (e.g. 0.5 = 50%)\n",
    "    test_size = fraction of the full dataset becoming the test set (e.g. 0.5 = 50%)\n",
    "    \"\"\"\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    # get total number of data points\n",
    "    n_total = X.shape[0]\n",
    "    \n",
    "    # Permutate X and y\n",
    "    perm = np.random.permutation(n_total)\n",
    "    \n",
    "    # Compute the number of data points for training, validation, and test set.\n",
    "    n_test, n_val = int(np.floor(n_total*test_size)), int(np.floor(n_total*val_size))\n",
    "    n_train = n_total - n_test - n_val\n",
    "    print(f\"Data points in training set: {n_train}, validation set: {n_val}, test set: {n_test}\")\n",
    "    \n",
    "    # Obtain the indices corresponding to the train, val & test set.\n",
    "    # With the indices, split X & y into the three sets each.\n",
    "    idcs_train = perm[0:n_train]\n",
    "    idcs_val   = perm[n_train:n_train+n_val]\n",
    "    idcs_test  = perm[n_train+n_val:]\n",
    "    \n",
    "    # split X\n",
    "    X_train    = X[idcs_train]\n",
    "    X_val      = X[idcs_val]\n",
    "    X_test     = X[idcs_test]\n",
    "    \n",
    "    # split y\n",
    "    y_train    = y[idcs_train]\n",
    "    y_val      = y[idcs_val]\n",
    "    y_test     = y[idcs_test]\n",
    "    \n",
    "    # return all\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, idcs_train, idcs_val, idcs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab359b4-dd00-4745-9db2-cf1d8cab5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up scalers and scale data\n",
    "xscaler, yscaler = StandardScaler(), StandardScaler()\n",
    "xit = xscaler.inverse_transform\n",
    "yit = yscaler.inverse_transform\n",
    "X, y = xscaler.fit_transform(measurements), yscaler.fit_transform(defect_locs[:,None]).reshape(-1)\n",
    "\n",
    "# Run the function you created\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, _, _, idcs_test = train_test_val_split(X,y, test_size=0.2, val_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63780463-8b17-425f-bb6b-05bddcce653c",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7b4c1-f7bb-40cc-8531-0be9ec16bcc1",
   "metadata": {},
   "source": [
    "First, let's try a linear model with linear features. We use the `MLPRegressor` to stay consistent with the workflow for the rest of the notebook. A linear model with linear features can be obtained by setting the activation function to be the identity. The inputs are multiplied with the weights twice &mdash; once before and once after the hidden layer. However, the linear combination of linear models will still result in a linear model, which makes this little trick work. The training is trivial, and we therefore employ the built-in `MLPRegressor.fit()` function to this end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b735f-96f2-4e5b-830a-060dfceec309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up linear regression model\n",
    "LinearModel = MLPRegressor(solver='sgd', hidden_layer_sizes=(10), activation='identity', learning_rate='constant')\n",
    "\n",
    "# train NN\n",
    "LinearModel.fit(X_train, y_train)\n",
    "y_pred = LinearModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcf9da-c18c-47d6-a126-492b157ad812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sensor indeces\n",
    "idcs = (3,7) # <-- Change this to change sensor data considered\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# create figure\n",
    "fig, ax = plt.subplots(1,3,figsize=(12.5,3.8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# collect data\n",
    "X_plot = xit(X_test)[:,idcs]\n",
    "\n",
    "# plot\n",
    "plot0 = ax[0].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_test[:,None]).reshape(-1))\n",
    "plot1 = ax[1].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(X_plot[:,0], X_plot[:,1], c=np.abs(y_pred-y_test))\n",
    "\n",
    "# adjust plots\n",
    "format_colorbar_plot(fig, ax, [plot0, plot1, plot2], idcs)\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f495d-2e9a-4caf-b9d9-208bb034e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the sample in the test set we want to look at\n",
    "index = 74 # <--- Change this index to change the sample!\n",
    "total_idx = idcs_test[index]\n",
    "\n",
    "# get prediction and true value of the defect location\n",
    "defect_loc_true = yit(y_test[[index]][None,:])[0,0]\n",
    "defect_loc_pred = yit(LinearModel.predict(X_test[[index],:])[:,None])[0,0]\n",
    "\n",
    "# create the plot\n",
    "plotly_plot(df, total_idx, measure_locs, defect_loc_true, defect_loc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1cf46-a1f9-4734-a075-70e9a1b4fbcb",
   "metadata": {},
   "source": [
    "This is not awful, but we can do better then that. Let's try a nonlinear model now! As you might already have guessed form or implementation of the linear model, we propose a neural network. We also discard the `MPLRegressor.fit()` function and implement the training loop our selves, to be able to take a look under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b234a70-8578-46f8-8bbb-301c484f3a9c",
   "metadata": {},
   "source": [
    "# Network training\n",
    "With our dataset ready, we can create a function to train a neural network (NN). There are many choices to be made when creating such a function, most are related to the bias - variance tradeoff discussed in notebook 2. The outline of the function we want to create is as follows:\n",
    "\n",
    "`def NN_train()`:<br>\n",
    "&emsp;&emsp; For a maximum number of epochs:<br>\n",
    "&emsp;&emsp;&emsp;&emsp; Permutate the data<br> \n",
    "&emsp;&emsp;&emsp;&emsp; For every minibatch:<br> \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Collect the X & y training data<br> \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Take a NN step (partial fit)<br> \n",
    "&emsp;&emsp;&emsp;&emsp; Compute the root mean squared error (RMSE) on the validation set<br> \n",
    "&emsp;&emsp;&emsp;&emsp; If the RMSE is the lowest:<br> \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Save the RMSE & NN model<br> \n",
    "&emsp;&emsp;&emsp;&emsp; If RMSE has not decreased in the last X epochs<br> \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Stop the training loop<br> \n",
    "&emsp;&emsp;&emsp;&emsp; Adapt the learning rate if necessary<br> \n",
    "&emsp;&emsp; Return NN, rmse_min & full_rmse_array<br>\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "    Go through the training loop algorithm and compare with the code. The code is **not** complete. Identify what bits are missing and implement them.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a88f42-78c2-4c9e-94f7-0002c3abb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the NN\n",
    "def NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=100000, tol=1e-6, verbose=False, lr_init=1e-2, lr_pow=0.9, lr_step=500, seed=0, batchsize=50):\n",
    "    \n",
    "    # set seed \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    # set up array for mse and improvement count for early stopping\n",
    "    rmse = [] \n",
    "    rmse_min = 1e10\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # init learning rate\n",
    "    lr = lr_init\n",
    "    NN.learning_rate_init = lr\n",
    "    \n",
    "    NN_best = NN\n",
    "    \n",
    "    # loop over iterations\n",
    "    for epoch in range(max_epoch):\n",
    "            \n",
    "        # set up permutation of the data\n",
    "        n = X_train.shape[0]\n",
    "        perm = np.random.permutation(n)\n",
    "        batches_per_epoch = int(np.floor(n/batchsize))\n",
    "        \n",
    "        # loop over batches\n",
    "        for it in range(batches_per_epoch):\n",
    "            \n",
    "            # collect current batch\n",
    "            X_batch = X_train[perm[it*batchsize:(it+1)*batchsize]]\n",
    "            y_batch = y_train[perm[it*batchsize:(it+1)*batchsize]]\n",
    "                \n",
    "            # take step\n",
    "            NN.partial_fit(X_batch, y_batch)\n",
    "            \n",
    "        # compute rmse on validation set after each epoch\n",
    "        y_val_hat = NN.predict(X_val)\n",
    "        rmse.append(np.sqrt(np.sum((y_val - y_val_hat)**2)))\n",
    "        \n",
    "        # adapt learning rate\n",
    "        if (epoch > 0) and (epoch%lr_step==0):\n",
    "            lr *= lr_pow\n",
    "            NN.learning_rate_init = lr\n",
    "            if verbose:\n",
    "                print(\"Reduced learning rate to {:.4e}\".format(lr))\n",
    "        \n",
    "        # check if no improvement occured in last iters\n",
    "        if rmse[-1] - rmse_min > tol:\n",
    "            no_improvement_count += 1\n",
    "        elif rmse[-1] < rmse_min:\n",
    "            rmse_min = rmse[-1]\n",
    "            no_improvement_count = 0\n",
    "            NN_best = NN\n",
    "        \n",
    "        # exit loop when no improvement was registered during past twenty iters\n",
    "        if no_improvement_count == 20:\n",
    "            print(\"Training stopped after {} epochs\".format(epoch))\n",
    "            break\n",
    "        \n",
    "        # print loss (optional)\n",
    "        if verbose and epoch%200==0:\n",
    "            print(\"\\nIteration {}\".format(epoch))\n",
    "            print(\"   rmse {:.4e}\\n\".format(rmse[epoch]))\n",
    "    \n",
    "    if (epoch==max_epoch-1): print(\"Reached max_epochs ( {} )\".format(max_epoch))\n",
    "    \n",
    "    # return trained network and last rmse\n",
    "    return NN_best, rmse_min, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98715035-4775-4b1e-b2f0-b1c61adf573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NN\n",
    "NN = MLPRegressor(solver='sgd', hidden_layer_sizes=(5, 5), activation='tanh', learning_rate='constant')\n",
    "\n",
    "# train NN\n",
    "NN, _, rmse = NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=10000, verbose=True, lr_init=1e-1, lr_step=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29eec5e-030d-48ac-9e3c-b7bd049f059f",
   "metadata": {},
   "source": [
    "You can use the following plotting routines to visualize your predictions. Keep in mind that all of the following graphs are based on projections of the input data on 1D or 2D subspaces that suppress at least part of the information contained in the dataset. Those projections, however, are necessary to enable visualizations of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9044744-0196-4a9f-8d9e-55f6c6e9b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3 # <-- Change this to change sensor data considered\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# get prediction\n",
    "y_pred = NN.predict(X_test)\n",
    "\n",
    "# create figure and select component\n",
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "\n",
    "# plot data\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_test[:,None]).reshape(-1), label='truth', s=30)\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_pred[:,None]).reshape(-1), label='prediction', s=30)\n",
    "\n",
    "# adjust plot\n",
    "ax.legend()\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax.set_xlabel(rf\"$u_{{{int(idx/2)+1}, {'x' if idx%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_ylabel(r'$x_{defect}$')\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a838f-e77a-44e1-b351-fbff0bdc7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "idcs = (3,7) # <-- Change this to change sensor data considered\n",
    "\n",
    "# plot prediciton for projection of inputs on 2D subspace\n",
    "# ======================================== ignore code ==============================================================\n",
    "# create figure and select measurements to plot\n",
    "fig, ax = plt.subplots(1,3,figsize=(12.5,3.8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# collect data\n",
    "X_plot = xit(X_test)[:,idcs]\n",
    "\n",
    "# plot\n",
    "plot0 = ax[0].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_test[:,None]).reshape(-1))\n",
    "plot1 = ax[1].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(X_plot[:,0], X_plot[:,1], c=np.abs(y_pred-y_test))\n",
    "\n",
    "# adjust plots\n",
    "format_colorbar_plot(fig, ax, [plot0, plot1, plot2], idcs)\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb947589-c9e0-47dd-b687-ee55bf205cf2",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c55058-fa36-4466-9216-0bf224aceaac",
   "metadata": {},
   "source": [
    "The network's performance looks good on a visual inspection, but we need to quantify the error and compare it for different architectures to find the best-performing model. For this purpose, we turn to a grid-search strategy to find hyperparameters that give the best prediction on a validation set.\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "- Create arrays over the hyperparameters to vary\n",
    "- Loop over the arrays\n",
    "- Initialize & train the NN\n",
    "- Compare the RMSE of different models\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\\\n",
    "Note: For the parameters, at least vary the number of layers and the size of each layer. Optionally, also look at the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bb37e-41db-41b6-bebc-0f0305899ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define coordinate vectors for grid\n",
    "layer_sizes = [5,10, 15, 20]\n",
    "layer_numbers = [1, 2, 3, 4]\n",
    "\n",
    "# get grid for the coordinate pairs and store them in an array\n",
    "rmse = np.zeros((len(layer_sizes), len(layer_numbers)))\n",
    "\n",
    "# loop all hidden layer sizes\n",
    "for i, lsize in enumerate(layer_sizes):\n",
    "    \n",
    "    # loop over all numbers of hidden layers\n",
    "    for j, lnumber in enumerate(layer_numbers):\n",
    "    \n",
    "        # get tuple for archbatch_size=cture and print\n",
    "        layers = (lsize,) * lnumber\n",
    "        print(\"Training NN with hidden layers:  {}\".format(layers))\n",
    "        \n",
    "        # get NN\n",
    "        NN = MLPRegressor(solver='sgd', hidden_layer_sizes=layers, activation='tanh')\n",
    "        NN, rmse[i,j], _ = NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=100000, verbose=False, lr_init=1e-1, lr_step=20)\n",
    "        \n",
    "        # print\n",
    "        print(\"     Mean square error:    {:.4e}\\n\".format(rmse[i,j]))\n",
    "\n",
    "\n",
    "# get NN that gave lowerst rmse and print\n",
    "min_size, min_number = np.unravel_index(np.argmin(rmse), rmse.shape)\n",
    "print(\"\\n\\nModel with {} layers and {} neurons per layer gave lowest rmse of {:.4e}\".format(layer_numbers[min_number], layer_sizes[min_size], rmse[min_size, min_number]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748746c-b741-407a-a803-303f2832328c",
   "metadata": {},
   "source": [
    "## Model prediction on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d618dd-aef9-4931-bbac-072fe5f5820f",
   "metadata": {},
   "source": [
    "Let's use our test data to visualize our best-performing model and test its predictive capabilities. First, re-initialize & train the model with the optimal hyperparameters.\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "- Obtain the parameters that lead to the best RMSE and retrain the model.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be98f3-8b56-4615-be11-b540e142b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NN\n",
    "layers = (layer_sizes[min_size],) * layer_numbers[min_number]\n",
    "NN = MLPRegressor(solver='sgd', hidden_layer_sizes=layers, activation='tanh')\n",
    "\n",
    "# train NN\n",
    "NN, _, _ = NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=10000, verbose=False, lr_init=1e-1, lr_step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03eb7ee-9863-4229-a58d-53631b6fafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "y_pred = NN.predict(X_test)\n",
    "\n",
    "# select sensor component\n",
    "idx = 3 # <-- Change this to change sensor data considered\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "\n",
    "# plot data\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_test[:,None]).reshape(-1), label='truth', s=30)\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_pred[:,None]).reshape(-1), label='prediction', s=30)\n",
    "\n",
    "# adjust plot\n",
    "ax.legend()\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax.set_xlabel(rf\"$u_{{{int(idx/2)+1}, {'x' if idx%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_ylabel(r'$x_{defect}$')\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1591b-38da-47d4-b429-7d11d0663c23",
   "metadata": {},
   "source": [
    "Let's plot the prediction for projection of inputs on 2D subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64d301-684e-4324-a17c-a2699c46cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sensor indeces\n",
    "idcs = (3,7) # <-- Change this to change sensor data considered\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# create figure\n",
    "fig, ax = plt.subplots(1,3,figsize=(12.5,3.8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# collect data\n",
    "X_plot = xit(X_test)[:,idcs]\n",
    "\n",
    "# plot\n",
    "plot0 = ax[0].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_test[:,None]).reshape(-1))\n",
    "plot1 = ax[1].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(X_plot[:,0], X_plot[:,1], c=np.abs(y_pred-y_test))\n",
    "\n",
    "# adjust plots\n",
    "format_colorbar_plot(fig, ax, [plot0, plot1, plot2], idcs)\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054afdbd-5505-41d9-a170-0afdf9af3f20",
   "metadata": {},
   "source": [
    "Let us pick a few data points (or samples/bridges) from our test set to inspect how well our predictions compare with the ground truth. Change the index at the top of the following code block to change the sample, you can ignore the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a97c52-0c84-4af0-a730-aef2692fd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the sample in the test set we want to look at\n",
    "index = 51 # <--- Change this index to change the sample!\n",
    "total_idx = idcs_test[index]\n",
    "\n",
    "# get prediction and true value of the defect location\n",
    "defect_loc_true = yit(y_test[[index]][None,:])[0,0]\n",
    "defect_loc_pred = yit(NN.predict(X_test[[index],:])[:,None])[0,0]\n",
    "\n",
    "# create the plot\n",
    "plotly_plot(df, total_idx, measure_locs, defect_loc_true, defect_loc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091dcb4-431a-4818-a02b-8c6555e39a02",
   "metadata": {},
   "source": [
    "Finally, we need to compute the RMSE for all samples in the test set to quantify our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8ac9d-e3bb-4076-8d76-931ca201f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = NN.predict(X_test)\n",
    "rmse_test = np.sqrt(np.sum((yit(y_pred_test[:,None]) - yit(y_test[:,None])).reshape(-1)**2) / y_test.shape[0])\n",
    "print(\"RMSE on test set for best performing model: {:.4e}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e9db7-cac6-4a90-be71-f4c0470b4b7b",
   "metadata": {},
   "source": [
    "## Feature selection: Beyond individual sensors\n",
    "So far, we have used our engineering judgement to pick a subset of sensor locations. With these sensors, we have been able to make fairly accurate predictions for the defect location. Choosing a subset of all available sensor locations was necessary to keep the number of inputs feasible, and to keep the cost of the sensors low.\n",
    "Alternatively if we have a sensor in each location, we can use Principal Component Analysis (PCA), to reduce the information from all sensors into a few modes. For a recap on PCA you can review previous MUDE lectures.\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "- Create a dataset with the dx and dy data from all sensors\n",
    "- Use PCA to transform the dataset into 10 features per sample. The number 10 is chosen to keep the number of inputs to the network the same compared to 5 sensors with x & y data.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f2842-3780-4b11-b37c-cf9bcf1b0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Obtain full dataset\n",
    "measurements = df[['dx','dy']].to_numpy().flatten()\n",
    "measurements = np.reshape(measurements, (1000, -1))  # Shape: [Num_samples(1000) x features]\n",
    "\n",
    "# Creating PCA modes\n",
    "# -------------------\n",
    "num_modes = 10\n",
    "# -------------------\n",
    "pca = PCA(n_components=num_modes)\n",
    "pca.fit(measurements)\n",
    "\n",
    "measurement_modes = pca.transform(measurements)\n",
    "\n",
    "print( f\"The variance explained by each component =\\n{pca.explained_variance_ratio_}\\n\")\n",
    "print( f\"The singular values =\\n{pca.singular_values_}\\n\")\n",
    "\n",
    "print(f\"Number of features without PCA: {len(measurements[0])}\")\n",
    "print(f\"Number of features with PCA: {len(measurement_modes[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab0275-d9b2-4d61-990e-25b78510f3bb",
   "metadata": {},
   "source": [
    "### Visualizing PCA modes\n",
    "Similar to how we can plot the defect location depending on the individual sensors above, we can also plot the location based on the PCA modes. This is done in the following section. Select different modes to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ba359-858f-4fa3-9139-545b8c7eff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the modes to plot\n",
    "modes = (0, 1) # <--- Change this index to change the pca modes to plot!\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plot1 = ax.scatter(measurement_modes[:,modes[0]], measurement_modes[:,modes[1]], c=defect_locs[:], s=40)\n",
    "format_colorbar_pca_plot(fig, ax, plot1, modes)\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece23191-eb76-4bd6-8b60-7384c7c572ff",
   "metadata": {},
   "source": [
    "Similarly to earlier, we need to further pre-process our data.\n",
    "\n",
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "    Re-normalize and split the data. (Note that only the inputs X change with PCA, the output stays the same)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffde60-259e-422a-b432-4701f8826e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up scalers and scale data\n",
    "xscaler_pca = StandardScaler()  # Scaler for y does not change\n",
    "xit_pca = xscaler_pca.inverse_transform\n",
    "X_pca = xscaler_pca.fit_transform(measurement_modes)\n",
    "\n",
    "# Split into train, validation & test set. Permutation is the same, so we only need to obtain new X\n",
    "X_train_pca, X_val_pca, X_test_pca, y_train_pca, y_val_pca, y_test_pca, _, _, idcs_test_pca = train_test_val_split(X_pca, y, test_size=0.2, val_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9d49f-1a25-473c-8768-54d2a68df9a3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; vertical-align: middle; padding:3px 20px;\">\n",
    "<p>\n",
    "    \n",
    "<b>Task:</b>\n",
    "Initialize & train a single NN model based on this data.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8307d85-c81d-477f-99d1-9153c949f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NN\n",
    "NN_PCA = MLPRegressor(solver='sgd', hidden_layer_sizes=(15, 15, 15), activation='tanh')\n",
    "\n",
    "# train NN\n",
    "NN_PCA, _, _ = NN_train(NN_PCA, X_train_pca, y_train_pca, X_val_pca, y_val_pca, max_epoch=10000, verbose=False, lr_init=1e-1, lr_step=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4d4f4-9e5e-47fe-982f-49b87923abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 # <-- Change this to change sensor data considered\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# get prediction\n",
    "y_pred_pca = NN_PCA.predict(X_test_pca)\n",
    "X_plot = xscaler_pca.inverse_transform(X_test_pca)\n",
    "\n",
    "# create figure and select component\n",
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "\n",
    "# plot data\n",
    "ax.scatter(X_plot[:,idx], yit(y_test_pca[:,None]).reshape(-1), label='truth', s=30)\n",
    "ax.scatter(X_plot[:,idx], yit(y_pred_pca[:,None]).reshape(-1), label='prediction', s=30)\n",
    "\n",
    "# adjust plot\n",
    "ax.legend()\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax.set_xlabel(rf\"$u_{{{int(idx/2)+1}, {'x' if idx%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_ylabel(r'$x_{defect}$')\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30c25e-962f-4c36-ac82-576998e09b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sensor indeces\n",
    "modes = (0,1) # <-- Change this to change sensor data considered\n",
    "\n",
    "# ======================================== ignore code ==============================================================\n",
    "# create figure\n",
    "fig, ax = plt.subplots(1,3,figsize=(12,3.8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# collect data\n",
    "X_plot = xscaler_pca.inverse_transform(X_test_pca)[:,modes]\n",
    "\n",
    "# plot\n",
    "plot0 = ax[0].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_test_pca[:,None]))\n",
    "plot1 = ax[1].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred_pca[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(X_plot[:,0], X_plot[:,1], c=np.abs(yit(y_pred_pca[:,None]) - yit(y_test_pca[:,None])))\n",
    "\n",
    "# adjust plots\n",
    "format_colorbar_pca_plot(fig, ax, [plot0, plot1, plot2], modes)\n",
    "plt.show()\n",
    "# ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b623ae-9dfa-422a-9de9-011f0e553c6d",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "In the plot above, you can compare the values of the rightmost plot with the same plot made earlier with sensors.\n",
    "Lets see how the PCA predictions compare to those with manual sensors. Change the index at the top of the following code block to change the sample, you can ignore the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7db943-a80a-45eb-a517-4bb93ebcc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the sample in the test set we want to look at\n",
    "index = 51 # <--- Change this index to change the sample!\n",
    "total_idx = idcs_test[index]\n",
    "\n",
    "# get prediction and true value of the defect location\n",
    "defect_loc_true = yit(y_test[[index]][None,:])[0,0]\n",
    "defect_loc_pred = yit(NN.predict(X_test[[index],:])[:,None])[0,0]\n",
    "defect_loc_pred_pca = yit(NN_PCA.predict(X_test_pca[[index],:])[:,None])[0,0]\n",
    "\n",
    "plotly_plot(df, total_idx, measure_locs, defect_loc_true, defect_loc_pred, defect_loc_pred_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd726b46-d4c6-45db-b111-ff620fd5ef22",
   "metadata": {},
   "source": [
    "Finally, we need to compute the RMSE for all samples in the test set to quantify our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc18cb8-1b6e-416d-9e0b-b9b797a6ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test_pca = np.sqrt(np.sum((yit(y_test_pca[:,None]) - yit(y_pred_pca[:,None])).reshape(-1)**2) / yit(y_pred_pca[:,None]).shape[0])\n",
    "\n",
    "print(\"RMSE on test set for this PCA model: {:.4e}\".format(rmse_test_pca))\n",
    "print(\"RMSE on test set for the best performing model based on 5 sensors: {:.4e}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09509e2c-416a-418e-9cde-35fa416a4e45",
   "metadata": {},
   "source": [
    "Compare the error obtained using PCA with that using manual sensors.\n",
    "\n",
    "Optional: \n",
    "- How does the error change when using more or less PCA modes?\n",
    "- Do a hyperparameter study to find the best network when using PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdedb77-1c28-43ef-bba5-179fcc6049c1",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- Instead of using PCA, use K-means clustering to reduce the dimensionality of the problem. Can you obtain a lower error with it? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
