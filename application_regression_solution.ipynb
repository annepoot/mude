{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d585d2-6e7a-4a30-b3a6-0921ae399e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7464e1-b028-49e2-a515-b29ced4922f5",
   "metadata": {},
   "source": [
    "## General remark\n",
    "\n",
    "**This notebook contains a lot of code, especially for plotting. You do not need to understand all of it, try to focus on the parts that are hightlighted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb1954-141e-4b94-9d3d-28126aafd13f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you should apply the techniques you learned for regression in a more realistic problem setting. We have a collection of bridges modeled as 2D beams that all feature one defect. Our goal is to train a model to learn the location of this defect as a function of displacement measurements. Since sensors are expensive, we can only place them in five locations on the bridges. The assignment includes the following tasks:\n",
    "\n",
    "- Select five locations for the sensors based on a visual inspection of the displacement field \n",
    "- Pre-process the available data to use it in a neural network\n",
    "- Train a neural network to learn a mapping from the displacement measurements to the defect location, comment on the choice of hyperparameters (number of hidden layers, nodes per layer, ...)\n",
    "- Visualise your results and evaluate the accuracy of your network\n",
    "- Implement an alternative based on PCA where we have sensor data at every location\n",
    "\n",
    "Let's take a look at the dataset first. It is a CSV file, and a convenient way to read and manipulate this file type is via the `Dataframe` of the `pandas` library. Printing a few lines of the dataset before performing the analysis is good practice. We load the dataset into a `Dataframe` from the `pandas` library and print a few rows from the top and bottom. The dataset consists of a collection of displacement fields of the bridges. We have a total of 1000 bridges, as can be seen from the tail of the data frame `df.tail()`, and 712 locations in which the displacements have been measured, as can be seen from the tail of a sample `bar_0.tail()`, which is just a single sample we took from the dataset. Note that the location is uniform for a specific sample owing to the dataset's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe7b1c-eff1-40aa-a190-f0b04f02f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regression-data_realistic.csv')\n",
    "bar_0 = df[df['sample'] == 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5334850-43fe-4f9c-b0f6-9439c930d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecae500-dce6-442a-8dd3-87e493137355",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_0.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cb1a6-a0ef-42f6-84fe-7ef24c99bf60",
   "metadata": {},
   "source": [
    "## Data visualization and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00df669-8188-4d24-aadd-96d3fb8ab23c",
   "metadata": {},
   "source": [
    "Your first task is to select measurement locations. The following cell plots the displacement in the x- or y-direction or the magnitude over the beam's domain. You can select the three components via the buttons on the topside. In addition, you can see all the available measurement locations. You can hover over the plot, which will display the data frame's corresponding node ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bdb08-07a4-4a5f-8d9e-f8d6a1c2d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data corresponding to one single bridge, and interpolate the displacements on a grid for plotting\n",
    "bar_0 = df[df['sample'] == 0]\n",
    "grid_x, grid_y = np.mgrid[0.02:9.98:250j, 0.02:1.98:50j]\n",
    "grid_z = griddata(bar_0[['x','y']].to_numpy(), np.sqrt(bar_0['dx']**2 + bar_0['dy']**2), (grid_x, grid_y))\n",
    "\n",
    "# plot displacement-field and nodes\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z=grid_z.transpose(),\n",
    "                         x=grid_x[:,0],\n",
    "                         y=grid_y[0],\n",
    "                         hoverinfo='skip',\n",
    "                         name='heatmap'))\n",
    "\n",
    "# plot nodes\n",
    "fig.add_trace(go.Scatter(x=bar_0['x'],\n",
    "                         y=bar_0['y'],\n",
    "                         mode='markers',\n",
    "                         marker_color='black',\n",
    "                         name='',\n",
    "                         hovertemplate='<b>Node</b>: %{text}',\n",
    "                         text=bar_0['node']))\n",
    "\n",
    "# add buttons to display different displacement fields\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar_0[['x','y']].to_numpy(), np.sqrt(bar_0['dx']**2 + bar_0['dy']**2), (grid_x, grid_y)).transpose()]],\n",
    "                    label='magnitude', method='restyle'),\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar_0[['x','y']].to_numpy(), bar_0['dx'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='x',\n",
    "                    method='restyle'),\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar_0[['x','y']].to_numpy(), bar_0['dy'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='y',\n",
    "                    method='restyle')\n",
    "            ]),\n",
    "            direction='right', pad={'r': 10, 't': 10}, showactive=True, x=0.5, xanchor='left', y=1.1,\n",
    "            yanchor='bottom', type='buttons', font=dict(size=13)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add annotation for button\n",
    "fig.add_annotation(dict(font=dict(size=13), x=0.5, y=1.13, showarrow=False,\n",
    "                   xref='paper', yref='paper', xanchor='right', yanchor='bottom', text='Displacement: '))\n",
    "\n",
    "# update xaxis range and show figure\n",
    "fig.update_xaxes(range=(-0.2,10.2), constrain='domain')\n",
    "fig.update_yaxes(range=(-0.2,2.2), constrain='domain', scaleanchor='x', scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e267d18-7f85-41f7-b356-7a6f86ec5462",
   "metadata": {},
   "source": [
    "Select five measurement locations that you expect to be informative. Plug them into the predefined list `measure_locs`. Remember that we only have a budget of five locations, make sure to not exceed this threshold to secure a spot on the leaderboard (more to that later). The remaining code in this cell collects the displacements from all of our beams at the selected nodes and the defect location. These quantities are stored in the arrays `measurements` and `defect_locs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8385552-fb55-4633-a86f-1be8f0970c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define measurement locations, get corresponding coord\n",
    "measure_locs = [481, 461, 312, 41, 66]\n",
    "measure_coords = np.array([bar_0[bar_0['node'] == loc][['x','y']].to_numpy() for loc in measure_locs]).squeeze(1)\n",
    "\n",
    "# double check the measurement locations\n",
    "print(measure_coords)\n",
    "                             \n",
    "# read measurement from all samples in dataframe\n",
    "measurements = np.empty((df['sample'].max()+1,0))\n",
    "\n",
    "# loop through measurement locations and collect measuments from all samples\n",
    "for loc in measure_locs:\n",
    "    dx = df[df['node'] == loc]['dx'].to_numpy()\n",
    "    dy = df[df['node'] == loc]['dy'].to_numpy()\n",
    "    measurements = np.append(measurements, np.vstack((dx, dy)).transpose(),axis=1)\n",
    "\n",
    "# get defect locations\n",
    "defect_locs = df[df['node']==0]['location'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc9f7f-bffc-4b12-9ad4-9ee0ae1d60f0",
   "metadata": {},
   "source": [
    "Let's plot the defect location as a function of the displacement measurements to get a feel for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601760f2-2ef4-4444-b7bd-cbbc2693f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few hyperplanes of the dataset\n",
    "fig, ax = plt.subplots(5,2, figsize=(10,20))\n",
    "[ax.flat[i].scatter(measurements[:,i], defect_locs, s=5) for i in range(len(ax.flat))]\n",
    "[ax.flat[i].ticklabel_format(style='sci', axis='x', scilimits=(0,0)) for i in range(len(ax.flat))]\n",
    "[ax[0,i].set_title(title) for i,title in enumerate([r'$u_x$',r'$u_y$'])]\n",
    "[ax[i,0].text(-0.4, 0.46, r'node {}'.format(i+1), transform=ax[i,0].transAxes, fontsize=12) for i in range(ax.shape[0])]\n",
    "[ax[i,0].set_ylabel(r'$x_{defect}$') for i in range(ax.shape[0])]\n",
    "ticks = [np.linspace(np.min(measurements[:,i]), np.max(measurements[:,i]), 4) for i in range(measurements.shape[1])]\n",
    "[axs.set_xticks(tick) for axs, tick in zip(ax.flat, ticks)]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d51b6-bd38-41a9-9ab9-4c172d9d276c",
   "metadata": {},
   "source": [
    "We can see that most measurements do not have a unique mapping to the defect location, suggesting we need multiple features to distinguish between the deformation states.\n",
    "\n",
    "**Task**: Change the measurement locations (2 code cells above this text) and study the influence on the plots. Finally, pick 5 sensors you believe will lead to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac55d5-81d1-4040-8601-a63b3d62d589",
   "metadata": {},
   "source": [
    "Let's take a look at a 2D scatterplot of our data. Note that this is a projection of the data on this particular 2D subspace of the input space. The color bar indicates the defect location of a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387855e1-ba3f-472f-9961-601e4ff81d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select components to plot\n",
    "#   remeber that we flattened the measurement array, meaning that\n",
    "#   even indices correspond to x-measurements, uneven incdices to y-measurements.\n",
    "#   the tuple (3,7) threfore corresponds to u_y in node 2, and u_y in node 4\n",
    "#   you can also look at the plot to see which component you are inspecting\n",
    "\n",
    "idcs = (3,7)\n",
    "skip = 5\n",
    "\n",
    "# make figure and plot data\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plot1 = ax.scatter(measurements[::skip,idcs[0]], measurements[::skip,idcs[1]], c=defect_locs[::skip], s=39)\n",
    "[ax.ticklabel_format(style='sci', axis=axis, scilimits=(0,0)) for axis in ['x','y']]\n",
    "ax.set_xlabel(rf\"$u_{{{int(idcs[0]/2)+1}, {'x' if idcs[0]%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_ylabel(rf\"$u_{{{int(idcs[1]/2)+1}, {'x' if idcs[1]%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_xticks(np.linspace(np.min(measurements[::skip,idcs[0]]), np.max(measurements[::skip,idcs[0]]), 4))\n",
    "fig.colorbar(plot1)\n",
    "ax.set_title(\"Defect location (color) as a function \\n of two measurements\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcef26c-2520-4574-9036-b3efe61feb74",
   "metadata": {},
   "source": [
    "This should look more promising; the defect location seems to be an injective function when considering multiple measurements.\n",
    "\n",
    "**Task**: Select different indices to be plotted. Compare choosing only x components with choosing y components. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1a5e7-bd37-44fb-a69d-53b674e6129f",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16f4ed-3c96-4d24-a36c-13df30a43dbf",
   "metadata": {},
   "source": [
    "To train a model using this data, we need to split it in a training, validation and test set. The training set is used to train the model, the validation set is used to fit the hyperparameters of the NN, and the test set is used to assess the predictive capabilities of the resulting model. The latter should not be used for model selection or training. We need truly unseen data to properly evaluate the performance of our final model.\n",
    "\n",
    "**Task**:\n",
    "Complete the code below to implement a function that creates a random train, validation & test set.\n",
    "- Apply a random permutation to the data. Make sure X & y are permutated in the same manner!\n",
    "- Compute the size of the train, validation & test split\n",
    "- Select the permutated data splits\n",
    "- Select sensible fraction sizes for the validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c9915-c6ec-47bd-86eb-7936a8c202f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split dataset into training, validation, and test set\n",
    "def train_test_val_split(X, y, val_size, test_size, seed=0):\n",
    "    \"\"\"\n",
    "    X = [N x features]\n",
    "    y = [N x outputs]\n",
    "    val_size = fraction of the full dataset becoming the validation set (e.g. 0.5 = 50%)\n",
    "    test_size = fraction of the full dataset becoming the test set (e.g. 0.5 = 50%)\n",
    "    \"\"\"\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    # get total number of data points\n",
    "    n_total = X.shape[0]\n",
    "    \n",
    "    # Permutate X and y\n",
    "    ### CODE HERE ###\n",
    "    perm = np.random.permutation(n_total)\n",
    "    X, y = X[perm], y[perm]\n",
    "    ######\n",
    "    \n",
    "    # Compute the number of data points for training, validation, and test set.\n",
    "    ### CODE HERE ### \n",
    "    n_test, n_val = int(np.floor(n_total*test_size)), int(np.floor(n_total*val_size))\n",
    "    n_train = n_total - n_test - n_val\n",
    "    ######\n",
    "    print(f\"Data points in training set: {n_train}, validation set: {n_val}, test set: {n_test}\")\n",
    "    \n",
    "    # Obtain the indices corresponding to the train, val & test set.\n",
    "    # With the indices, split X & y into the three sets each.\n",
    "    ### CODE HERE ### \n",
    "    idcs_train = perm[0:n_train]\n",
    "    idcs_val   = perm[n_train:n_train+n_val]\n",
    "    idcs_test  = perm[n_train+n_val:]\n",
    "    \n",
    "    # split X\n",
    "    X_train    = X[idcs_train]\n",
    "    X_val      = X[idcs_val]\n",
    "    X_test     = X[idcs_test]\n",
    "    \n",
    "    # split y\n",
    "    y_train    = y[idcs_train]\n",
    "    y_val      = y[idcs_val]\n",
    "    y_test     = y[idcs_test]\n",
    "    ######\n",
    "    \n",
    "    # return all\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, idcs_train, idcs_val, idcs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab359b4-dd00-4745-9db2-cf1d8cab5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up scalers and scale data\n",
    "xscaler, yscaler = StandardScaler(), StandardScaler()\n",
    "yit = yscaler.inverse_transform\n",
    "xit = xscaler.inverse_transform\n",
    "X, y = xscaler.fit_transform(measurements), yscaler.fit_transform(defect_locs[:,None]).reshape(-1)\n",
    "\n",
    "# Run the function you created\n",
    "### CODE HERE ###\n",
    "X_train, X_val, X_test, y_train,y_val,y_test, _, _, idcs_test = train_test_val_split(X,y, test_size=0.2, val_size=0.2)\n",
    "###### (To keep naming convention for future plots, only remove stuff after the \"=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3531d65-c81a-469e-b631-17da12511fdc",
   "metadata": {},
   "source": [
    "# Network training\n",
    "With our dataset ready, we can create a function to train a neural network (NN). There are many choices to be made when creating such a function, most are related to the bias - variance tradeoff discussed in notebook 2. The outline of the function we want to create is as follows:\n",
    "\n",
    "def NN_train():<br>\n",
    "&emsp; For a maximum number of epochs:<br>\n",
    "&emsp;&emsp; Permutate the data<br> \n",
    "&emsp;&emsp; For every minibatch:<br> \n",
    "&emsp;&emsp;&emsp; Collect the X & y training data<br> \n",
    "&emsp;&emsp;&emsp; Take a NN step (partial fit)<br> \n",
    "&emsp;&emsp; Compute the root mean squared error (RMSE) on the validation set<br> \n",
    "&emsp;&emsp; If the RMSE is the lowest:<br> \n",
    "&emsp;&emsp;&emsp; Save the RMSE & NN model<br> \n",
    "&emsp;&emsp; If RMSE has not decreased in the last X epochs<br> \n",
    "&emsp;&emsp;&emsp; Stop the training loop<br> \n",
    "&emsp;&emsp; Adapt the learning rate if necessary<br> \n",
    "&emsp; Return NN, last_rmse (TODO: last or final?) & full_rmse_array<br>\n",
    "\n",
    "**Task**: Implement the above training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a88f42-78c2-4c9e-94f7-0002c3abb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the NN\n",
    "def NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=100000, tol=1e-6, verbose=False, lr_init=1e-2, lr_pow=0.9, lr_step=500, seed=0, batchsize=50):\n",
    "    \n",
    "    # set seed \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    # set up array for mse and improvement count for early stopping\n",
    "    rmse = [] \n",
    "    rmse_min = 1e10\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # init learning rate\n",
    "    lr = lr_init\n",
    "    NN.learning_rate_init = lr\n",
    "    \n",
    "    ### CODE HERE ###\n",
    "    # loop over iterations\n",
    "    for epoch in range(max_epoch):\n",
    "            \n",
    "        # set up permutation of the data\n",
    "        n = X_train.shape[0]\n",
    "        perm = np.random.permutation(n)\n",
    "        batches_per_epoch = int(np.floor(n/batchsize))\n",
    "        \n",
    "        # loop over batches\n",
    "        for it in range(batches_per_epoch):\n",
    "            \n",
    "            # collect current batch\n",
    "            X_batch = X_train[perm[it*batchsize:(it+1)*batchsize]]\n",
    "            y_batch = y_train[perm[it*batchsize:(it+1)*batchsize]]\n",
    "                \n",
    "            # take step\n",
    "            NN.partial_fit(X_batch, y_batch)\n",
    "            \n",
    "        # compute rmse on validation set after each epoch\n",
    "        y_val_hat = NN.predict(X_val)\n",
    "        rmse.append(np.sqrt(np.sum((y_val - y_val_hat)**2)))\n",
    "        \n",
    "        # adapt learning rate\n",
    "        if (epoch > 0) and (epoch%lr_step==0):\n",
    "            lr *= lr_pow\n",
    "            NN.learning_rate_init = lr\n",
    "            if verbose:\n",
    "                print(\"Reduced learning rate to {:.4e}\".format(lr))\n",
    "        \n",
    "        # check if no improvement occured in last iters\n",
    "        if rmse[-1] - rmse_min > tol:\n",
    "            no_improvement_count += 1\n",
    "        elif rmse[-1] < rmse_min:\n",
    "            rmse_min = rmse[-1]\n",
    "            no_improvement_count = 0\n",
    "        \n",
    "        # exit loop when no improvement was registered during past twenty iters\n",
    "        if no_improvement_count == 20:\n",
    "            print(\"Training stopped after {} epochs\".format(epoch))\n",
    "            break\n",
    "        \n",
    "        # print loss (optional)\n",
    "        if verbose and epoch%200==0:\n",
    "            print(\"\\nIteration {}\".format(epoch))\n",
    "            print(\"   rmse {:.4e}\\n\".format(rmse[epoch]))\n",
    "    \n",
    "    if (epoch==max_epoch-1): print(\"Reached max_epochs ( {} )\".format(max_epoch))\n",
    "    ######\n",
    "    \n",
    "    # return trained network and last rmse\n",
    "    return NN, rmse[-1], rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98715035-4775-4b1e-b2f0-b1c61adf573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NN\n",
    "NN = MLPRegressor(solver='sgd', hidden_layer_sizes=(5, 5), activation='tanh', learning_rate='constant')\n",
    "\n",
    "# train NN\n",
    "NN, _, rmse = NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=10000, verbose=True, lr_init=1e-1, lr_step=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29eec5e-030d-48ac-9e3c-b7bd049f059f",
   "metadata": {},
   "source": [
    "You can use the following plotting routines to visualize your predictions. Keep in mind that all of the following graphs are based on projections of the input data on 1D or 2D subspaces that suppress at least part of the information contained in the dataset. Those projections, however, are necessary to enable visualizations of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9044744-0196-4a9f-8d9e-55f6c6e9b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3 # <-- Change this to change sensor data considered\n",
    "\n",
    "# get prediction\n",
    "y_pred = NN.predict(X_test)\n",
    "\n",
    "# create figure and select component\n",
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "\n",
    "# plot data\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_test[:,None]).reshape(-1), label='truth', s=30)\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_pred[:,None]).reshape(-1), label='prediction', s=30)\n",
    "\n",
    "# adjust plot\n",
    "ax.legend()\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax.set_xlabel(rf\"$u_{{{int(idx/2)+1}, {'x' if idx%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_ylabel(r'$x_{defect}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a838f-e77a-44e1-b351-fbff0bdc7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "idcs = (3,7) # <-- Change this to change sensor data considered\n",
    "\n",
    "# plot prediciton for projection of inputs on 2D subspace\n",
    "# create figure and select measurements to plot\n",
    "fig, ax = plt.subplots(1,3,figsize=(12.5,3.8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# collect data\n",
    "X_plot = xit(X_test)[:,idcs]\n",
    "x_ticks = np.linspace(np.min(X_plot[:,0]), np.max(X_plot[:,0]), 4)\n",
    "\n",
    "# plot\n",
    "plot0 = ax[0].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot1 = ax[1].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(X_plot[:,0], X_plot[:,1], c=np.abs(y_pred-y_test))\n",
    "\n",
    "# adjust plots\n",
    "[plt.colorbar(plot, ax=ax[i]) for i, plot in enumerate([plot0, plot1, plot2])]\n",
    "[ax[i].set_title(title) for i, title in enumerate([r'true $y$', r'prediction $\\hat y$', r'$|y - \\hat y|$'])]\n",
    "[axs.ticklabel_format(style='sci', axis=axis, scilimits=(0,0)) for axis in ['x','y'] for axs in ax]\n",
    "[axs.set_xticks(x_ticks) for axs in ax]\n",
    "[axs.set_xlabel(rf\"$u_{{{int(idcs[0]/2)+1}, {'x' if idcs[0]%2 == 0 else 'y'}}}$\", fontsize=12) for axs in ax]\n",
    "ax[0].set_ylabel(rf\"$u_{{{int(idcs[1]/2)+1}, {'x' if idcs[1]%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb947589-c9e0-47dd-b687-ee55bf205cf2",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9074eca-d2c9-4e7d-88eb-ac066541d1d2",
   "metadata": {},
   "source": [
    "The network's performance looks good on a visual inspection, but we need to quantify the error and compare it for different architectures to find the best-performing model. For this purpose, we turn to a grid-search strategy to find hyperparameters that give the best prediction on a validation set.\n",
    "\n",
    "**Task**:\n",
    "- Create arrays over the parameters to vary\n",
    "- Loop over every array\n",
    "- Initialize & train the NN\n",
    "- Compare the RMSE of different models\n",
    "\n",
    "Note: For the parameters, at least vary the number of layers and the size of each layer. Optionally, also look at the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bb37e-41db-41b6-bebc-0f0305899ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### CODE HERE ###\n",
    "# define coordinate vectors for grid\n",
    "layer_sizes = [5,10, 15, 20]\n",
    "layer_numbers = [1, 2, 3, 4]\n",
    "\n",
    "# get grid for the coordinate pairs and store them in an array\n",
    "rmse = np.zeros((len(layer_sizes), len(layer_numbers)))\n",
    "\n",
    "# loop all hidden layer sizes\n",
    "for i, lsize in enumerate(layer_sizes):\n",
    "    \n",
    "    # loop over all numbers of hidden layers\n",
    "    for j, lnumber in enumerate(layer_numbers):\n",
    "    \n",
    "        # get tuple for archbatch_size=cture and print\n",
    "        layers = (lsize,) * lnumber\n",
    "        print(\"Training NN with hidden layers:  {}\".format(layers))\n",
    "        \n",
    "        # get NN\n",
    "        NN = MLPRegressor(solver='sgd', hidden_layer_sizes=layers, activation='tanh')\n",
    "        NN, rmse[i,j], _ = NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=100000, verbose=False, lr_init=1e-1, lr_step=20)\n",
    "        \n",
    "        # print\n",
    "        print(\"     Mean square error:    {:.4e}\\n\".format(rmse[i,j]))\n",
    "\n",
    "\n",
    "# get NN that gave lowerst rmse and print\n",
    "min_size, min_number = np.unravel_index(np.argmin(rmse), rmse.shape)\n",
    "print(\"\\n\\nModel with {} layers and {} neurons per layer gave lowest rmse of {:.4e}\".format(layer_numbers[min_number], layer_sizes[min_size], rmse[min_size, min_number]))\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748746c-b741-407a-a803-303f2832328c",
   "metadata": {},
   "source": [
    "## Model prediction on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693efb86-a3df-4c87-974b-668d71767250",
   "metadata": {},
   "source": [
    "Let's use our test data to visualize our best-performing model and test its predictive capabilities. First, re-initialize & train the model with the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be98f3-8b56-4615-be11-b540e142b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n",
    "# Set up NN\n",
    "layers = (layer_sizes[min_size],) * layer_numbers[min_number]\n",
    "NN = MLPRegressor(solver='sgd', hidden_layer_sizes=layers, activation='tanh')\n",
    "\n",
    "# train NN\n",
    "NN, _, _ = NN_train(NN, X_train, y_train, X_val, y_val, max_epoch=10000, verbose=False, lr_init=1e-1, lr_step=50)\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03eb7ee-9863-4229-a58d-53631b6fafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "y_pred = NN.predict(X_test)\n",
    "\n",
    "# create figure and select component\n",
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "idx = 3\n",
    "\n",
    "# plot data\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_test[:,None]).reshape(-1), label='truth', s=30)\n",
    "ax.scatter(xit(X_test)[:,idx], yit(y_pred[:,None]).reshape(-1), label='prediction', s=30)\n",
    "\n",
    "# adjust plot\n",
    "ax.legend()\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax.set_xlabel(rf\"$u_{{{int(idx/2)+1}, {'x' if idx%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "ax.set_ylabel(r'$x_{defect}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64d301-684e-4324-a17c-a2699c46cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediciton for projection of inputs on 2D subspace\n",
    "# create figure and select measurements to plot\n",
    "fig, ax = plt.subplots(1,3,figsize=(12.5,3.8), constrained_layout=True, sharey=True)\n",
    "idcs = (3,7)\n",
    "\n",
    "# collect data\n",
    "X_plot = xit(X_test)[:,idcs]\n",
    "x_ticks = np.linspace(np.min(X_plot[:,0]), np.max(X_plot[:,0]), 4)\n",
    "\n",
    "# plot\n",
    "plot0 = ax[0].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot1 = ax[1].scatter(X_plot[:,0], X_plot[:,1], c=yit(y_pred[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(X_plot[:,0], X_plot[:,1], c=np.abs(y_pred-y_test))\n",
    "\n",
    "# adjust plots\n",
    "[plt.colorbar(plot, ax=ax[i]) for i, plot in enumerate([plot0, plot1, plot2])]\n",
    "[ax[i].set_title(title) for i, title in enumerate([r'true $y$', r'prediction $\\hat y$', r'$|y - \\hat y|$'])]\n",
    "[axs.ticklabel_format(style='sci', axis=axis, scilimits=(0,0)) for axis in ['x','y'] for axs in ax]\n",
    "[axs.set_xticks(x_ticks) for axs in ax]\n",
    "[axs.set_xlabel(rf\"$u_{{{int(idcs[0]/2)+1}, {'x' if idcs[0]%2 == 0 else 'y'}}}$\", fontsize=12) for axs in ax]\n",
    "ax[0].set_ylabel(rf\"$u_{{{int(idcs[1]/2)+1}, {'x' if idcs[1]%2 == 0 else 'y'}}}$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054afdbd-5505-41d9-a170-0afdf9af3f20",
   "metadata": {},
   "source": [
    "Let us pick a few data points (or samples/bridges) from our test set to inspect how well our predictions compare with the ground truth. Change the index at the top of the following code block to change the sample, you can ignore the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5e9a6-2065-40b9-accd-debfddbd1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data corresponding to one single bridge, an interpolate the displacements on a grid for plotting\n",
    "index = 51 # <--- Change this index to change the sample!\n",
    "total_idx = idcs_test[index]\n",
    "\n",
    "# get corresponding bar, displacement fields and measurement location coordinates\n",
    "bar = df[df['sample'] == total_idx]\n",
    "measure_coords = np.array([bar[bar['node'] == loc][['x','y']].to_numpy() for loc in measure_locs]).squeeze(1)\n",
    "grid_x, grid_y = np.mgrid[0.02:9.98:250j, 0.02:1.98:50j]\n",
    "grid_z = griddata(bar[['x','y']].to_numpy(), np.sqrt(bar['dx']**2 + bar['dy']**2), (grid_x, grid_y))\n",
    "\n",
    "# # plot displacement field and nodes\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z=grid_z.transpose(), x=grid_x[:,0], y=grid_y[0],\n",
    "                         hoverinfo='skip', name='heatmap'))\n",
    "\n",
    "# add buttons for additinal fields\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar[['x','y']].to_numpy(), np.sqrt(bar['dx']**2 + bar['dy']**2), (grid_x, grid_y)).transpose()]],\n",
    "                    label='magnitude', method='restyle'),\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar[['x','y']].to_numpy(), bar['dx'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='x',\n",
    "                    method='restyle'),\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar[['x','y']].to_numpy(), bar['dy'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='y',\n",
    "                    method='restyle')\n",
    "            ]),\n",
    "            direction='right', pad={'r': 10, 't': 10}, showactive=True, x=0.5, xanchor='left', y=1.2,\n",
    "            yanchor='bottom', type='buttons', font=dict(size=13)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add annotation for button\n",
    "fig.add_annotation(dict(font=dict(size=13), x=0.5, y=1.23, showarrow=False,\n",
    "                   xref='paper', yref='paper', xanchor='right', yanchor='bottom', text='Displacement: '))\n",
    "\n",
    "# plot measurement locations\n",
    "fig.add_trace(go.Scatter(x = measure_coords[:,0], y = measure_coords[:,1], mode='markers',\n",
    "                         marker=dict(size=10, color='DarkSlateGrey', line=dict(width=2, color='white')),\n",
    "                         hovertemplate='<b>Node</b>: %{text}', text=measure_locs, name=''))\n",
    "\n",
    "# get prediction and true value for defect location\n",
    "defect_loc_pred = yit(NN.predict(X_test[[index],:])[:,None])[0,0]\n",
    "defect_loc_true = yit(y_test[[index]][None,:])[0,0]\n",
    "\n",
    "# plot vertical lines at the two locations\n",
    "fig.add_vline(x=defect_loc_pred, name='pred', line=dict(color='LightSlateGrey'))\n",
    "fig.add_vline(x=defect_loc_true, name='truth', line=dict(color='LightSlateGrey'), line_dash='dot')\n",
    "\n",
    "# put labels on the lines\n",
    "fig.add_annotation(dict(font=dict(size=13), x=defect_loc_pred, y=1.15, showarrow=False,\n",
    "                   xref='x', yref='paper', text='prediction: {:.2f}'.format(defect_loc_pred)))\n",
    "fig.add_annotation(dict(font=dict(size=13), x=defect_loc_true, y=-.22, showarrow=False,\n",
    "                   xref='x', yref='paper', text='truth: {:.2f}'.format(defect_loc_true)))\n",
    "                        \n",
    "# update axis and show figure\n",
    "fig.update_xaxes(range=(-0.2,10.2), constrain='domain')\n",
    "fig.update_yaxes(range=(-0.2,2.2), constrain='domain', scaleanchor='x', scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091dcb4-431a-4818-a02b-8c6555e39a02",
   "metadata": {},
   "source": [
    "Finally, we need to compute the RMSE for all samples in the test set to quantify our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8ac9d-e3bb-4076-8d76-931ca201f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = NN.predict(X_test)\n",
    "rmse_test = np.sqrt(np.sum((yit(y_pred_test[:,None]) - yit(y_test[:,None])).reshape(-1)**2) / y_test.shape[0])\n",
    "print(\"RMSE on test set for best performing model: {:.4e}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245064cb-3f52-4df4-81ce-3deea90cd2e2",
   "metadata": {},
   "source": [
    "## Feature selection: Beyond individual sensors\n",
    "So far, we have used our engineering judgement to pick a subset of sensor locations. With these sensors, we have been able to make fairly accurate predictions for the defect location. Choosing a subset of all available sensor locations was necessary to keep the number of inputs feasible, and to keep the cost of the sensors low.\n",
    "Alternatively if we have a sensor in each location, we can use Principal Component Analysis (PCA), to reduce the information from all sensors into a few modes. For a recap on PCA you can review previous MUDE lectures.\n",
    "\n",
    "**Task**:\n",
    "- Create a dataset with the dx and dy data from all sensors\n",
    "- Use PCA to transform the dataset into 10 features per sample. The number 10 is chosen to keep the number of inputs to the network the same compared to 5 sensors with x & y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f2842-3780-4b11-b37c-cf9bcf1b0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here: Create a dataset of PCA modes\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Code here ##\n",
    "\n",
    "# Obtain full dataset\n",
    "measurements = df[['dx','dy']].to_numpy().flatten()\n",
    "measurements = np.reshape(measurements, (1000, -1))  # Shape: [Num_samples(1000) x features]\n",
    "\n",
    "# Creating PCA modes\n",
    "# -------------------\n",
    "num_modes = 10\n",
    "# -------------------\n",
    "pca = PCA(n_components=num_modes)\n",
    "pca.fit(measurements)\n",
    "\n",
    "X_reduced = pca.transform(measurements)\n",
    "###### (To keep naming convention for future plots, only remove stuff after the \"=\" for X_reduced & pca)\n",
    "\n",
    "print( f\"The variance explained by each component = {pca.explained_variance_ratio_}\\n\")\n",
    "print( f\"The singular values = {pca.singular_values_}\\n\")\n",
    "\n",
    "print(f\"Number of features without PCA: {len(measurements[0])}\")\n",
    "print(f\"Number of features with PCA: {len(X_reduced[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab0275-d9b2-4d61-990e-25b78510f3bb",
   "metadata": {},
   "source": [
    "### Visualizing PCA modes\n",
    "Similar to how we can plot the defect location depending on the individual sensors above, we can also plot the location based on the PCA modes. This is done in the following section. Select different modes to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ba359-858f-4fa3-9139-545b8c7eff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the modes to plot\n",
    "modes = [0, 1]\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "plot1 = ax.scatter(X_reduced[:,modes[0]], X_reduced[:,modes[1]], c=defect_locs[:], s=40)\n",
    "[ax.ticklabel_format(style='sci', axis=axis, scilimits=(0,0)) for axis in ['x','y']]\n",
    "ax.set_xlabel(f'PCA mode {modes[0]}')\n",
    "ax.set_ylabel(f'PCA mode {modes[1]}')\n",
    "ax.set_title(\"Defect location (color) as a function \\n of two PCA modes\")\n",
    "fig.colorbar(plot1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f80ed1-482d-495f-8236-8dab457d4814",
   "metadata": {},
   "source": [
    "Similarly to earlier, we need to further pre-process our data.\n",
    "\n",
    "**Task**: Re-normalize and split the data. (Note that only the inputs X change with PCA, the output stays the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffde60-259e-422a-b432-4701f8826e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n",
    "# set up scalers and scale data\n",
    "xscaler= StandardScaler()  # Scaler for y does not change\n",
    "X = xscaler.fit_transform(X_reduced)\n",
    "\n",
    "# Split into train, validation & test set\n",
    "X_train_pca, X_val_pca, X_test_pca, y_train_pca,y_val_pca,y_test_pca, _, _, idcs_test = train_test_val_split(X,y, test_size=0.2, val_size=0.2)\n",
    "###### (To keep naming convention for future plots, only remove stuff after the \"=\" for split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa197f6-0b53-477d-a88c-1a68e964c30a",
   "metadata": {},
   "source": [
    "**Task**: Initialize & train a single NN model based on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8307d85-c81d-477f-99d1-9153c949f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n",
    "# Set up NN\n",
    "NN_PCA = MLPRegressor(solver='sgd', hidden_layer_sizes=(15, 15, 15), activation='tanh')\n",
    "\n",
    "# train NN\n",
    "NN_PCA, _, _ = NN_train(NN_PCA, X_train_pca, y_train_pca, X_val_pca, y_val_pca, max_epoch=10000, verbose=False, lr_init=1e-1, lr_step=100 )\n",
    "###### (To keep naming convention for future plots, only remove stuff after the \"=\" for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d414c5-e25b-4772-a8ad-965b530c48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pca = NN_PCA.predict(X_test_pca)\n",
    "\n",
    "x_out = xscaler.inverse_transform(X_test_pca)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "ax.scatter(x_out[:,0], yit(y_test_pca[:,None]), label='truth', s=30)\n",
    "ax.scatter(x_out[:,0], yit(y_pred_pca[:,None]), label='predition', s=30)\n",
    "ax.legend()\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax.set_xlabel(r'$u$')\n",
    "ax.set_ylabel(r'$x_{defect}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30c25e-962f-4c36-ac82-576998e09b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(12,3.8), constrained_layout=True, sharey=True)\n",
    "plot0 = ax[0].scatter(x_out[:,modes[0]], x_out[:,modes[1]], c=yit(y_test_pca[:,None]))\n",
    "plot1 = ax[1].scatter(x_out[:,modes[0]], x_out[:,modes[1]], c=yit(y_pred_pca[:,None]).reshape(-1))\n",
    "plot2 = ax[2].scatter(x_out[:,modes[0]], x_out[:,modes[1]], c=np.abs(yit(y_pred_pca[:,None])-yit(y_test_pca[:,None])))\n",
    "\n",
    "# adjust plots\n",
    "[plt.colorbar(plot, ax=ax[i]) for i, plot in enumerate([plot0, plot1, plot2])]\n",
    "[ax[i].set_title(title) for i, title in enumerate([r'true $y$', r'prediction $\\hat y$', r'$|y - \\hat y|$'])]\n",
    "[axs.ticklabel_format(style='sci', axis=axis, scilimits=(0,0)) for axis in ['x','y'] for axs in ax]\n",
    "[axs.set_xlabel(rf\"mode {modes[0]}\") for axs in ax]\n",
    "ax[0].set_ylabel(rf\"mode {modes[1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b623ae-9dfa-422a-9de9-011f0e553c6d",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "In the plot above, you can compare the values of the rightmost plot with the same plot made earlier with sensors.\n",
    "Lets see how the PCA predictions compare to those with manual sensors. Change the index at the top of the following code block to change the sample, you can ignore the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d0194-df90-4a4b-9bfc-86945ea57687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data corresponding to one single bridge, an interpolate the displacements on a grid for plotting\n",
    "index = 51 # <--- Change this index to change the sample!\n",
    "total_idx = idcs_test[index]\n",
    "\n",
    "# get corresponding bar, displacement fields and measurement location coordinates\n",
    "bar = df[df['sample'] == total_idx]\n",
    "measure_coords = np.array([bar[bar['node'] == loc][['x','y']].to_numpy() for loc in measure_locs]).squeeze(1)\n",
    "grid_x, grid_y = np.mgrid[0.02:9.98:250j, 0.02:1.98:50j]\n",
    "grid_z = griddata(bar[['x','y']].to_numpy(), np.sqrt(bar['dx']**2 + bar['dy']**2), (grid_x, grid_y))\n",
    "\n",
    "# # plot displacement field and nodes\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z=grid_z.transpose(), x=grid_x[:,0], y=grid_y[0],\n",
    "                         hoverinfo='skip', name='heatmap'))\n",
    "\n",
    "# add buttons for additinal fields\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar[['x','y']].to_numpy(), np.sqrt(bar['dx']**2 + bar['dy']**2), (grid_x, grid_y)).transpose()]],\n",
    "                    label='magnitude', method='restyle'),\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar[['x','y']].to_numpy(), bar['dx'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='x',\n",
    "                    method='restyle'),\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar[['x','y']].to_numpy(), bar['dy'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='y',\n",
    "                    method='restyle')\n",
    "            ]),\n",
    "            direction='right', pad={'r': 10, 't': 10}, showactive=True, x=0.5, xanchor='left', y=1.2,\n",
    "            yanchor='bottom', type='buttons', font=dict(size=13)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add annotation for button\n",
    "fig.add_annotation(dict(font=dict(size=13), x=0.5, y=1.23, showarrow=False,\n",
    "                   xref='paper', yref='paper', xanchor='right', yanchor='bottom', text='Displacement: '))\n",
    "\n",
    "# plot measurement locations\n",
    "fig.add_trace(go.Scatter(x = measure_coords[:,0], y = measure_coords[:,1], mode='markers',\n",
    "                         marker=dict(size=10, color='DarkSlateGrey', line=dict(width=2, color='white')),\n",
    "                         hovertemplate='<b>Node</b>: %{text}', text=measure_locs, name=''))\n",
    "\n",
    "# get prediction and true value for defect location\n",
    "defect_loc_pred = yit(NN.predict(X_test[[index],:])[:,None])[0,0]\n",
    "defect_loc_pred_PCA = yit(NN_PCA.predict(X_test_pca[[index],:])[:,None])[0,0]\n",
    "defect_loc_true = yit(y_test[[index]][None,:])[0,0]\n",
    "\n",
    "# plot vertical lines at the two locations\n",
    "fig.add_vline(x=defect_loc_pred, name='pred', line=dict(color='LightSlateGrey'))\n",
    "fig.add_vline(x=defect_loc_pred_PCA, name='pred_PCA', line=dict(color='Black'))\n",
    "fig.add_vline(x=defect_loc_true, name='truth', line=dict(color='LightSlateGrey'), line_dash='dot')\n",
    "\n",
    "# put labels on the lines\n",
    "fig.add_annotation(dict(font=dict(size=13), x=defect_loc_pred, y=1.15, showarrow=False,\n",
    "                   xref='x', yref='paper', text='5 sensor prediction: {:.2f}'.format(defect_loc_pred)))\n",
    "fig.add_annotation(dict(font=dict(size=13), x=defect_loc_pred_PCA, y=1.25, showarrow=False,\n",
    "                   xref='x', yref='paper', text='PCA prediction: {:.2f}'.format(defect_loc_pred_PCA)))\n",
    "fig.add_annotation(dict(font=dict(size=13), x=defect_loc_true, y=-.22, showarrow=False,\n",
    "                   xref='x', yref='paper', text='truth: {:.2f}'.format(defect_loc_true)))\n",
    "                        \n",
    "# update axis and show figure\n",
    "fig.update_xaxes(range=(-0.2,10.2), constrain='domain')\n",
    "fig.update_yaxes(range=(-0.2,2.2), constrain='domain', scaleanchor='x', scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd726b46-d4c6-45db-b111-ff620fd5ef22",
   "metadata": {},
   "source": [
    "Finally, we need to compute the RMSE for all samples in the test set to quantify our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc18cb8-1b6e-416d-9e0b-b9b797a6ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test_pca = np.sqrt(np.sum((yit(y_test_pca[:,None]) - yit(y_pred_pca[:,None])).reshape(-1)**2) / yit(y_pred_pca[:,None]).shape[0])\n",
    "\n",
    "print(\"RMSE on test set for this PCA model: {:.4e}\".format(rmse_test_pca))\n",
    "print(\"RMSE on test set for the best performing model based on 5 sensors: {:.4e}\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09509e2c-416a-418e-9cde-35fa416a4e45",
   "metadata": {},
   "source": [
    "Compare the error obtained using PCA with that using manual sensors.\n",
    "\n",
    "Optional: \n",
    "- How does the error change when using more or less PCA modes?\n",
    "- Do a hyperparameter study to find the best network when using PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdedb77-1c28-43ef-bba5-179fcc6049c1",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- Instead of using PCA, use K-means clustering to reduce the dimensionality of the problem. Can you obtain a lower error with it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3f971-1545-4b45-ad2e-d2e6b9d4ad76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
