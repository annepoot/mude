{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mude_tools import neuralnetplotter\n",
    "from mude_tools import draw_neural_net\n",
    "%matplotlib widget"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "Recall that in the previous notebooks we have applied linear models with basis functions\n",
    "$$ y(x,\\mathbf{w}) = \\sum_{j=0}^M w_j \\phi_j(x) = \\mathbf{w}^T \\boldsymbol{\\phi} (x).$$\n",
    "\n",
    "Here $\\mathbf{w}$ are the flexible parameters, and $\\boldsymbol{\\phi}$ the basis functions.\n",
    "\n",
    "Because a linear model is linear in its parameters $\\mathbf{w}$, we could solve for $\\bar{\\mathbf{w}}$ directly\n",
    "\n",
    "$$ \\bar{\\mathbf{w}} = \\big( \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} \\big)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{t},$$\n",
    "\n",
    "where $\\mathbf{\\Phi}$ is the collection of basis funcitons evaluated in all data points. The basis function need to be choose *a priori* for this approach. When the phenomenon to be modeled is complex, relying on pre-defined basis functions might not give sufficient accuracy. We can overcome this issue with a more flexible model is required. Increase flexibility can be achieved by replacing the basis functions with parametric functions. In this notebook we will dive into one variant of this concept, namely neural networks. The underlying problem will stay the same: we are trying to learn a process based on a limited number of noisy observations $\\mathcal{D}=\\{\\mathbf{X}, \\mathbf{t}\\}$. Following decision theory, we need to minimize the mean squared error loss function\n",
    "\n",
    "$$\n",
    "E_D =  \\frac{1}{N} \\sum_{n=1}^N \\big(t_n - y(x_n, \\mathbf{w}) \\big)^2\n",
    "$$  <!--- \\dfrac{1}{2N} -->\n",
    "\n",
    "where $y(x, \\mathbf{w})$ now represents a neural network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The true function relating t to x\n",
    "def f_truth(x, freq=2, **kwargs):\n",
    "\n",
    "    # Return a sine with a frequency of f\n",
    "    return np.sin(x * freq)\n",
    "\n",
    "# The data generation function\n",
    "def f_data(epsilon=0.7, N=100, **kwargs):\n",
    "\n",
    "    # Apply a seed if one is given\n",
    "    if 'seed' in kwargs:\n",
    "        np.random.seed(kwargs['seed'])\n",
    "\n",
    "    # Get the minimum and maximum\n",
    "    xmin = kwargs.get('xmin', 0)\n",
    "    xmax = kwargs.get('xmax', 2*np.pi)\n",
    "\n",
    "    # Generate N evenly spaced observation locations\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "\n",
    "    # Generate N noisy observations (1 at each location)\n",
    "    t = f_truth(x, **kwargs) + np.random.normal(0, epsilon, N)\n",
    "\n",
    "    # Return both the locations and the observations\n",
    "    return x, t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural network architecture\n",
    "A neural network consists of neurons connected by weights, with information flowing from input neurons towards output neurons. The states of the input and output neurons are known during training. There are additional neurons in layers in between the inputs and outputs, forming a so-called hidden layer. Neurons are separated into layers, where all neurons of one layer depend on the neurons of the previous layer.\n",
    "\n",
    "The state of a neuron is determined by a linear combination of states $z$ from the previous layer with their connecting weights $w$\n",
    "\n",
    "$$\n",
    "a^{(l)}_{j} = \\sum_{i}^{D} w_{ji}^{(l)} z_{i}^{(l-1)} + w_{j0}^{(l)}\n",
    "$$\n",
    "\n",
    "where $w_{j0}^{(l)}$ are the biases, allowing the model to have an offset. Make sure not to confuse this quantity with the model bias. The linear combination of states is followed by a nonlinear transformation with an activation function $h(\\cdot)$:\n",
    "\n",
    "$$\n",
    "z^{(l)}_{j} = h(a^{(l)}_{j}).\n",
    "$$\n",
    "\n",
    "In the plot below, you can see the identity (or linear), sigmoid, hyperbolic tangent (tanh), and rectified linear unit (relu) activation functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4, 25)\n",
    "\n",
    "# Compute activation functions\n",
    "identity = x\n",
    "sigmoid = [1 / (1 + np.exp(-x)) for x in x]\n",
    "tanh = np.tanh(x)\n",
    "relu = [max(0, x) for x in x]\n",
    "\n",
    "# Plot figure\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x, identity, 'k-v',label='Identity (linear)')\n",
    "plt.plot(x, sigmoid, 'b-|',label='Sigmoid')\n",
    "plt.plot(x, tanh, 'g-o',label='Tanh')\n",
    "plt.plot(x, relu, 'r-x', label='ReLU') # c='red', linestyle=(0, (20, 30)),label='relu')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-2, 2)\n",
    "plt.xlabel('$a$')\n",
    "plt.ylabel('$z$')\n",
    "plt.title('Activation functions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of layers in a neural network commonly refers to the number of hidden layers. The formulation of a two-layer neural network is thus:\n",
    "\n",
    "$$\n",
    "y(x, \\mathbf{w}) = h^{(out)} \\left( \\sum_{k=1}^{K} w_{k}^{(3)} h^{(2)} \\bigg( \\sum_{j=1}^{J} w_{kj}^{(2)} h^{(1)} \\Big( \\sum_{i=1}^{I} w_{ji}^{(2)} x_i + w_{j0}^{(1)} \\Big) + w_{k0}^{(2)} \\bigg) + w_{0}^{(3)} \\right)\n",
    "$$\n",
    "\n",
    "With the nonlinear activation functions, the model is evidently no longer linear in the weights and, in general, no closed-form solution can be found. Instead, some sort of gradient-based optimization scheme, as discussed in the previous notebook in the form of SGD, is required calibrate the weights.\n",
    "\n",
    "When your dataset contains multiple inputs or outputs, this model can easily be extended by including multiple neurons in the input or output layer, the other procedures stay the same. Generally, the activation function of the outputs $h^{(out)}$ is linear, the activations in hidden layers are of a nonlinear type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8,3))\n",
    "draw_neural_net(ax[0], .1, .9, .1, .9, [1, 5, 1])\n",
    "draw_neural_net(ax[1], .1, .9, .1, .9, [2, 5, 3])\n",
    "ax[0].set_title('1 input, 5 hidden nodes, 1 output')\n",
    "ax[1].set_title('2 inputs, 5 hidden nodes, 3 outputs')\n",
    "[axs.axis('off') for axs in ax]\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model flexibility\n",
    "\n",
    "The neutral network's flexibility can be adapted by varying the number of neurons per hidden layer, or by adding more hidden layers. Both options lead to an increase number of parameters $\\mathbf{w}$. When a neural network has too little parameters, it generally puts us at risk at underfitting the data, whereas having too many parameters quickly leads to overfitting. The number of layers and neurons per layer are hyperparameters, which need to be calibrated. They are typically determined by fitting multiple models on hold-out data.\n",
    "\n",
    "In the following interactive plot you can study the influence of the number of neurons per layer on the model's prediciton. The number of hidden layers is fixed at two. You to to click the re-run button to retrain the model after varying the parameter. Be aware that the required computations take a few seconds."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the prediction locations\n",
    "# (note that these are different from the locations where we observed our data)\n",
    "x_pred = np.linspace(-1, 2*np.pi+1, 200)\n",
    "\n",
    "xscaler = StandardScaler()\n",
    "xscaler.fit(x[:,None])\n",
    "\n",
    "# Function that creates a NN\n",
    "def create_NN(**kwargs):\n",
    "    return MLPRegressor(solver='sgd', hidden_layer_sizes=(kwargs['neurons'], kwargs['neurons']),\n",
    "                        activation=kwargs['activation'], batch_size=kwargs['batch_size'])\n",
    "\n",
    "\n",
    "# Function that trains a given NN for a given number of epochs\n",
    "def NN_train(x, t, **kwargs):\n",
    "\n",
    "    # Get the network from the kwargs\n",
    "    network = kwargs.get('network')\n",
    "    \n",
    "    # Convert the training data to a column vector and normalize it\n",
    "    X = x.reshape(-1, 1)\n",
    "    X = xscaler.transform(X)\n",
    "\n",
    "    # Get the number of epochs per block\n",
    "    epochs_per_block = int(round(kwargs['epochs'] / kwargs['epoch_blocks'], 0))\n",
    "\n",
    "    # Run a number of epochs\n",
    "    for i in range(epochs_per_block):\n",
    "        network.partial_fit(X, t)\n",
    "\n",
    "    return network, network.loss_curve_\n",
    "\n",
    "\n",
    "# Function that returns predictions from a given NN model\n",
    "def NN_pred(x, t, x_pred, **kwargs):\n",
    "\n",
    "    # Get the network from the kwargs\n",
    "    network = kwargs.get('network')\n",
    "    return_network = kwargs.get('return_network', False)\n",
    "\n",
    "    if network is None:\n",
    "        network = create_NN(**kwargs)\n",
    "        kwargs['network'] = network\n",
    "        retrain = True\n",
    "    else:\n",
    "        retrain = kwargs.get('train_network', True)\n",
    "\n",
    "    # Convert the prediction data to a column vector and normalize it\n",
    "    X_pred = x_pred.reshape(-1, 1)\n",
    "    X_pred = xscaler.transform(X_pred)\n",
    "\n",
    "    if retrain:\n",
    "        network, train_loss = NN_train(x, t, **kwargs)\n",
    "\n",
    "    # Make a prediction at the locations given by x_pred\n",
    "    y = network.predict(X_pred)\n",
    "\n",
    "    if return_network:\n",
    "        return y, network, train_loss\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "plot1 = neuralnetplotter(f_data, f_truth, NN_pred, x_pred) # title=r'Run cell above plot before using!')\n",
    "plot1.add_sliders('neurons', valmax=20, valinit=3)\n",
    "plot1.add_buttons('truth', 'seed', 'rerun')\n",
    "# plot1.add_radiobuttons('activation')\n",
    "plot1.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you might have noticed, using the default setting of three neurons per layer was not enough for the network to learn the underlying trend in the data. Which number of neurons per layer gave you a visibly good fit?<!-- Does the validation error support your feeling?-->\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "Choosing a high number of neurons increases the number of parameters and, therefore, slows down training. In addition, it can make the model too flexible and prone to overfitting. It is therefore good practive to always monitor the predictive capability of a NN on a validation set. First, run the model below, then, select the model you think best fits the data by pulling the corresponding slider. At which epoch do you find the best model?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you might have noticed, using the default setting of three neurons per layer was not enough for the network to learn the underlying trend in the data. Which number of neurons per layer gave you a visibly good fit?<!-- Does the validation error support your feeling?-->\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "Choosing a high number of neurons increases the number of parameters and, therefore, slows down training. In addition, it can make the model too flexible and prone to overfitting. It is therefore good practive to always monitor the predictive capability of a NN on a validation set. First, run the model below, then, select the model you think best fits the data by pulling the corresponding slider. At which epoch do you find the best model?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot2 = neuralnetplotter(f_data, f_truth, create_NN, NN_train, NN_pred, x_pred)\n",
    "plot2.defaults['neurons']['valinit'] = 20\n",
    "plot2.defaults['N']['valinit'] = 40\n",
    "plot2.defaults['val_pct']['valinit'] = 60\n",
    "plot2.defaults['batch_size']['valinit'] = 2\n",
    "plot2.defaults['epochs']['valinit'] = 12000\n",
    "plot2.seed = 4\n",
    "plot2.add_sliders('cur_model')\n",
    "plot2.add_buttons('truth', 'rerun')\n",
    "plot2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is possible to train a neural network for a long time, and selecting one with a low validation error. An alternative, known as early stopping, uses the indication that the validation loss increases for a number of epochs as a stopping sign to halt training.\n",
    "\n",
    "Our strategy to train the network for a considerable amount of time and the select the one with the lowest validation error is valid, but not particularily efficient. An alternative known as early stopping constantly checks the validation error, and haults the training phase once an increase in said error (or its moving average) is detected. \n",
    "\n",
    "In a finite data setting, an increasing validation error does not give us certainty that we are staring to overfit our model, but it is a good indicator for this event. In our case, we know the process and have access to infinite data. For demonstration purposes, the true error of our model is computed alongside the validation error. If we draw a validation set that poorly represents our target, it can gives us confidence in our predictions that are very far off.\n",
    "\n",
    "<!--The true error is computed since the underlying model is known here. The validation error aims to approximate this true error, and in practice should be used to find the best model. The more data available, the more the training loss approximates the true loss. In the case plotted above, the minimum of the validation loss coincides with the minimum of the true loss.-->\n",
    "\n",
    "## Manual model selection\n",
    "\n",
    "In the previous notebook, you have seen how L<sub>2</sub>-regularization can be used to control the model complexity by introducting an additional term to the loss function:\n",
    "\n",
    "$$\n",
    "E = E_D + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "The application of this technique to neural networks is straightforward, and will therefore not be demonstrated in this notebook. Here, we will focus on the impact of the number of trainable parameters and the number of samples on overfitting. The ability to display our models at different stages of the training phase will help us to find and inspect particularily good or bad models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot3 = neuralnetplotter(f_data, f_truth, create_NN, NN_train, NN_pred, x_pred, nnlinewidth=0.25)\n",
    "plot3.add_sliders('freq', 'neurons', 'N', 'cur_model', 'val_pct')\n",
    "plot3.add_buttons('truth', 'seed', 'rerun')\n",
    "plot3.add_radiobuttons('activation')\n",
    "plot3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot3.defaults"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a number of different neural networks with varying hyperparameter settings, and try to understand the influence of all the the parameters on the resulting model. Try to answer the following questions:\n",
    "\n",
    "* Is the model with the lowest validation error always the model that, in your eyes, gives the best fit visually?\n",
    "* In practical situations when dealing with multiple inputs and outputs, it is often difficult to visualize the predictions for the whole domain. Can you detect when a model is underfit based only on the taining and validation loss?\n",
    "* For a well-trained flexible model with a large training size (N) the errors usually converge to a specific value. What is this value and why does this happen? What happens when the training error is below this value when using a small training size (N)?\n",
    "\n",
    "For a 2D demonstration that includes classification you can check out the [tensorflow playground][1].\n",
    "\n",
    "[1]: https://playground.tensorflow.org/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wrap-up\n",
    "In these lectures you have seen how non-parametric model, namely k-nearest neighbours, and have learned about the bias-variance trade-off. Linear regression was shown as a parametric model that is linear in its parameters and has a closed form solution. Ridge regression has been introduced to prevent overfitting. Stochastic gradient descent has been shown as a way to train a model in an iterative fashing. In this final notebook, we explored a model with a nonlinear relation with its parameters. You now understand the underlying principles of a broad set machine learning types, as well as how to distinguish pure curve fitting from extracting (or learning) the relevant trends and pattern in the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "9008b730",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is possible to train a neural network for a long time, and selecting one with a low validation error. An alternative, known as early stopping, uses the indication that the validation loss increases for a number of epochs as a stopping sign to halt training.\n",
    "\n",
    "Our strategy to train the network for a considerable amount of time and the select the one with the lowest validation error is valid, but not particularily efficient. An alternative known as early stopping constantly checks the validation error, and haults the training phase once an increase in said error (or its moving average) is detected. \n",
    "\n",
    "In a finite data setting, an increasing validation error does not give us certainty that we are staring to overfit our model, but it is a good indicator for this event. In our case, we know the process and have access to infinite data. For demonstration purposes, the true error of our model is computed alongside the validation error. If we draw a validation set that poorly represents our target, it can gives us confidence in our predictions that are very far off.\n",
    "\n",
    "<!--The true error is computed since the underlying model is known here. The validation error aims to approximate this true error, and in practice should be used to find the best model. The more data available, the more the training loss approximates the true loss. In the case plotted above, the minimum of the validation loss coincides with the minimum of the true loss.-->\n",
    "\n",
    "## Manual model selection\n",
    "\n",
    "In the previous notebook, you have seen how L<sub>2</sub>-regularization can be used to control the model complexity by introducting an additional term to the loss function:\n",
    "\n",
    "$$\n",
    "E = E_D + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "The application of this technique to neural networks is straightforward, and will therefore not be demonstrated in this notebook. Here, we will focus on the impact of the number of trainable parameters and the number of samples on overfitting. The ability to display our models at different stages of the training phase will help us to find and inspect particularily good or bad models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254be84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot3 = neuralnetplotter(f_data, f_truth, create_NN, NN_train, NN_pred, x_pred, nnlinewidth=0.25)\n",
    "plot3.add_sliders('freq', 'neurons', 'N', 'cur_model', 'val_pct')\n",
    "plot3.add_buttons('truth', 'seed', 'rerun')\n",
    "plot3.add_radiobuttons('activation')\n",
    "plot3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aaf298-4387-41e7-968f-87fc21f9935c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot3.defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebac5a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train a number of different neural networks with varying hyperparameter settings, and try to understand the influence of all the the parameters on the resulting model. Try to answer the following questions:\n",
    "\n",
    "* Is the model with the lowest validation error always the model that, in your eyes, gives the best fit visually?\n",
    "* In practical situations when dealing with multiple inputs and outputs, it is often difficult to visualize the predictions for the whole domain. Can you detect when a model is underfit based only on the taining and validation loss?\n",
    "* For a well-trained flexible model with a large training size (N) the errors usually converge to a specific value. What is this value and why does this happen? What happens when the training error is below this value when using a small training size (N)?\n",
    "\n",
    "For a 2D demonstration that includes classification you can check out the [tensorflow playground][1].\n",
    "\n",
    "[1]: https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4829586",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Wrap-up\n",
    "In these lectures you have seen how non-parametric model, namely k-nearest neighbours, and have learned about the bias-variance trade-off. Linear regression was shown as a parametric model that is linear in its parameters and has a closed form solution. Ridge regression has been introduced to prevent overfitting. Stochastic gradient descent has been shown as a way to train a model in an iterative fashing. In this final notebook, we explored a model with a nonlinear relation with its parameters. You now understand the underlying principles of a broad set machine learning types, as well as how to distinguish pure curve fitting from extracting (or learning) the relevant trends and pattern in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}