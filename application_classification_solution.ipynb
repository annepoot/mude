{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d585d2-6e7a-4a30-b3a6-0921ae399e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from matplotlib.widgets import Slider\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from classification_tools import manualMLPClassifier, NN_train_visualize, draw_neural_net\n",
    "\n",
    "%matplotlib widget\n",
    "# %matplotlib nbagg\n",
    "\n",
    "# Fix a seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86829be6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![overview beam structure](img/beam_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c1fee",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, you should apply the techniques you learned for classification in a more realistic problem setting. In the previous regression application, we predicted the position of a defect in the beam based on manually selected sensors. Here, we will try to classify the structural health of the beam. We are taking the extreme case of either requiring no maintenance or being so damaged that the beam needs to be demolished.\n",
    "\n",
    "First, we will use the data from two hand-picked sensors. After trying to get the best accuracy in this way, we will use turn to use all sensors simultaneously using dimensionality reduction. By carefully constructing the network, we can visualize what happens inside the network you trained. With that understanding, you will be tasked to find the best number of features to get the overall best prediction score.\n",
    "\n",
    "Let us start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe7b1c-eff1-40aa-a190-f0b04f02f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset that contains the (inherently noisy) observations\n",
    "df = pd.read_csv('classification-data_realistic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ee218",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The intervention, our target, is currently a string of either [unnecessary] or [demolition]. Using these classes with our computational tools requires converting them to a numerical value. In this 2-class problem we choose 0 & 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f3470",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Format all the data\n",
    "nodes_per_beam = df[df['sample'] == 0].shape[0]\n",
    "\n",
    "x = df[['dy']].to_numpy().flatten()\n",
    "X = np.reshape(x, (-1, nodes_per_beam))  # Shape: [Num_samples x num_features]\n",
    "\n",
    "# Replace the strings of our targets with numerical labels\n",
    "mapping = {'unnecessary' : 0, 'demolition' : 1}\n",
    "df_numeric = df.replace({'intervention' : mapping})     # Replace strings with integers\n",
    "damage_classes = df_numeric.iloc[::nodes_per_beam]['intervention']\n",
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00df669-8188-4d24-aadd-96d3fb8ab23c",
   "metadata": {},
   "source": [
    "## Data visualization and feature extraction\n",
    "Let's take a look at the beam again and all the locations for which we have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bdb08-07a4-4a5f-8d9e-f8d6a1c2d3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bar_0 = df[df['sample'] == 50]\n",
    "grid_x, grid_y = np.mgrid[0.02:9.98:250j, 0.02:1.98:50j]\n",
    "grid_z = griddata(bar_0[['x','y']].to_numpy(), bar_0['dy'], (grid_x, grid_y))\n",
    "\n",
    "# plot displacement-field and nodes\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z=grid_z.transpose(),\n",
    "                         x=grid_x[:,0],\n",
    "                         y=grid_y[0],\n",
    "                         hoverinfo='skip',\n",
    "                         name='heatmap'))\n",
    "\n",
    "# plot nodes\n",
    "fig.add_trace(go.Scatter(x=bar_0['x'],\n",
    "                         y=bar_0['y'],\n",
    "                         mode='markers',\n",
    "                         marker_color='black',\n",
    "                         name='',\n",
    "                         hovertemplate='<b>Node</b>: %{text}',\n",
    "                         text=bar_0['node']))\n",
    "\n",
    "# add buttons to display different displacement fields\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=['z', [griddata(bar_0[['x','y']].to_numpy(), bar_0['dy'], (grid_x, grid_y)).transpose()]],\n",
    "                    label='y',\n",
    "                    method='restyle')\n",
    "            ]),\n",
    "            direction='right', pad={'r': 10, 't': 10}, showactive=True, x=0.5, xanchor='left', y=1.1,\n",
    "            yanchor='bottom', type='buttons', font=dict(size=13)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add annotation for button\n",
    "fig.add_annotation(dict(font=dict(size=13), x=0.5, y=1.13, showarrow=False,\n",
    "                   xref='paper', yref='paper', xanchor='right', yanchor='bottom', text='Displacement: '))\n",
    "\n",
    "# update xaxis range and show figure\n",
    "fig.update_xaxes(range=(-0.2,10.2), constrain='domain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa08faa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can pick two sensors that measure the y displacement, which is used as the input to the classification model. You can find the number of the sensor by hovering over the above plot. The following section shows which sensors you picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415bd3f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define measurement locations, get corresponding coord\n",
    "# ----------------------------------------\n",
    "# measure_locs = [17, 73] # <- fill in the indices of the 2 sensors you select\n",
    "measure_locs = [29, 2] # <- fill in the indices of the 2 sensors you select\n",
    "# ----------------------------------------\n",
    "\n",
    "measure_coords = np.array([bar_0[bar_0['node'] == loc][['x','y']].to_numpy() for loc in measure_locs]).squeeze(1)\n",
    "\n",
    "measurements = X[:,measure_locs]\n",
    "\n",
    "bar = df[df['sample'] == 1]\n",
    "# measure_coords = np.array([bar[bar['node'] == loc][['x','y']].to_numpy() for loc in measure_locs]) #.squeeze(1)\n",
    "grid_x, grid_y = np.mgrid[0.02:9.98:250j, 0.02:1.98:50j]\n",
    "\n",
    "fig = go.Figure()\n",
    "# plot measurement locations\n",
    "fig.add_trace(go.Scatter(x=bar_0['x'],\n",
    "                         y=bar_0['y'],\n",
    "                         mode='markers',\n",
    "                         marker_size=4,\n",
    "                         marker_color='gray',\n",
    "                         name='',\n",
    "                         hovertemplate='<b>Node</b>: %{text}',\n",
    "                         text=bar_0['node']))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x = measure_coords[:,0], y = measure_coords[:,1], mode='markers',\n",
    "                         marker=dict(size=15, color='DarkSlateGrey', line=dict(width=2, color='white')),\n",
    "                         hovertemplate='<b>Node</b>: %{text}', text=measure_locs, name=''))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(range=(-0.2,10.2), constrain='domain')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07514bba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The two sensors record the y-displacement, here we can see how our data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604b3b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2,2,figsize=(10,10))\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "cmap = plt.get_cmap('RdYlGn_r', 2)\n",
    "\n",
    "plot1 = ax.scatter(measurements[:,0], measurements[:,1], c=damage_classes, cmap=cmap, vmin=-0.5, vmax=1.5, s=40)\n",
    "ax.set_xlabel(f'Sensor {measure_locs[0]} dy', labelpad=-30)\n",
    "ax.set_ylabel(f'Sensor {measure_locs[1]} dy', labelpad=-70)\n",
    "\n",
    "cax = plt.colorbar(plot1, ticks=list(mapping.values()))\n",
    "cax.ax.set_yticklabels(mapping.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ab3f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the above 2D plots, there is a large correlation between our inputs, and it is unclear to which extent we can train any model to separate these. Lets try to do it anyway! First we will normalize our data and split it into a training, validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ced62",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78710c8b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Properties required for classification\n",
    "indices = np.arange(measurements.shape[0])\n",
    "unique_classes = [0, 1]\n",
    "\n",
    "# Normalize\n",
    "xscaler = StandardScaler()\n",
    "# xit = xscaler.inverse_transform\n",
    "measurements_normalized = xscaler.fit_transform(measurements)\n",
    "\n",
    "# Split into train, validation & test data: 70% 15% 15%\n",
    "X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(measurements_normalized, damage_classes, indices, train_size=0.7)\n",
    "X_val, X_test, y_val, y_test, ind_val, ind_test = train_test_split(X_test, y_test, ind_test, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e766b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data is now ready, it is time to implement a neural network. **Excercise 1:** Use what you have learned in the notebook, and think about ways to identify as well as alleviate overfitting. You can adapt the template below and use the same techniques you applied in the regression application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28733f9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function to train the NN and obtain train & validation scores during training\n",
    "def NN_train(NN, X_train, y_train, X_val, y_val, epochs=1000, verbose=True, lr_init=1e-3):\n",
    "    validation_score = np.empty(epochs)\n",
    "    training_score = np.empty(epochs)\n",
    "\n",
    "    # set learning rate\n",
    "    NN.learning_rate_init = lr_init\n",
    "\n",
    "    # loop over iterations\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train for one epoch, compute the scores\n",
    "        NN.partial_fit(X_train, y_train, classes=unique_classes)\n",
    "\n",
    "        training_score[epoch] = NN.score(X_train, y_train)\n",
    "        validation_score[epoch] = NN.score(X_val, y_val)\n",
    "\n",
    "        # ----------------\n",
    "        # Model selection code\n",
    "        # ----------------\n",
    "\n",
    "        # print loss (optional)\n",
    "        if verbose and epoch%200==0:\n",
    "            print(f\"Iteration {epoch} out of {epochs}\")\n",
    "\n",
    "    # return trained network and last rmse\n",
    "    return NN, training_score, validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86940f63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NN = MLPClassifier(solver='adam', hidden_layer_sizes=(20, 20), activation='relu', random_state=1)\n",
    "# Optional: show we can use \"alpha=..\" in the line above to add regularization\n",
    "NN, training_score, validation_score = NN_train(NN, X_train, y_train, X_val, y_val, epochs=2000)\n",
    "\n",
    "# Plot your score\n",
    "print(f\"Train accuracy: {NN.score(X_train, y_train)}\")\n",
    "print(f\"Validation accuracy: {NN.score(X_val, y_val)}\")\n",
    "print(f\"Test accuracy: {NN.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fea9a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# plt.plot(NN.loss_curve_, label='training loss')\n",
    "plt.plot(training_score, label='Traininig')\n",
    "plt.plot(validation_score, label='Validation')\n",
    "plt.title('Score during training')\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylim(ymax=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score (Larger is better, maximum = 1)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec705ca9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How did the network do? Note that randomly picking classes would give an accuracy of 0.5, which we can use as a baseline score. If we have everything correct, the score would be 1. Lets visualize the predictions in 2D. In the plot below, circles are plotted, where the center of each circle corresponds to the true label, and the border corresponds to the predicted label. Thus if both are the same the network has predicted correctly. The background provides a contour of the networks predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71829304",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions based on the test set\n",
    "pred = NN.predict(X_test)\n",
    "\n",
    "# Make predictions for the decision boundary\n",
    "dx_points = 30\n",
    "x = np.linspace(-3.4, 3.4, dx_points)\n",
    "y = np.linspace(-3.4, 3.4, dx_points)\n",
    "xv, yv = np.meshgrid(x, y, indexing='xy')\n",
    "grid_points = np.append(xv.reshape(-1, 1), yv.reshape(-1, 1), axis=1)\n",
    "\n",
    "pred_grid = NN.predict(grid_points)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "prediction_color = cmap(pred/2)\n",
    "ax.contourf(xv, yv, pred_grid.reshape(xv.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "plot1 = ax.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=cmap, edgecolors=prediction_color, vmin=-0.5, vmax=1.5, s=60, linewidth=2, label='Center = True, Border = prediction')\n",
    "ax.legend()\n",
    "\n",
    "cax = plt.colorbar(plot1, ticks=list(mapping.values()))\n",
    "cax.ax.set_yticklabels(mapping.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c13ecd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature selection: Beyond 2 sensors\n",
    "Based on the plot above, it is clear that using the inputs of two sensors to predict the output is very challenging. To see if we can get a better performance, lets now use the data from all sensors. As this would give us a large number of inputs, our network would also become significantly larger. A neural network in the way we use it here is not capable of efficiently learning the large number of required parameters, and more data would be required than is available. To enable us to use all information available from the sensors we turn to dimensionality reduction, and specifically Principal Component Analysis (PCA). To shortly recap, PCA fits a number of straight lines (principal components, or modes) through all data, and these modes can then be combined to try to reconstruct the full space as close as possible. Each mode is determined by explaining as much variance as possible.\n",
    "\n",
    "### Dimensionality reduction and visualizing the inner working of a classification model\n",
    "Before letting you try to improve your classification score, lets take a step back and try to understand what is happening inside our model. We will do this visually, which is possible by only selecting two modes using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e30f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "num_modes = 2 # Keep this at 2 for now\n",
    "# -------------------\n",
    "pca = PCA(n_components=num_modes)\n",
    "pca.fit(X)\n",
    "print( f\"The variance explained by each component = {pca.explained_variance_ratio_}\")\n",
    "print( f\"The singular values = {pca.singular_values_}\")\n",
    "X_reduced = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a4287-db24-4e34-9ae4-fb39122e348d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets plot all our data in our two reduced PCA modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6157132",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r', 2)\n",
    "\n",
    "plot1 = ax.scatter(X_reduced[:,0], X_reduced[:,1], c=damage_classes, cmap=cmap, vmin=-0.5, vmax=1.5, s=40)\n",
    "cax = plt.colorbar(plot1, ticks=list(mapping.values()))\n",
    "cax.ax.set_yticklabels(mapping.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e5f00",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To visualize what is happening, we are constraining the final hidden layer of the neural network to only have two neurons, allowing us to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a271671",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "draw_neural_net(ax, .1, .9, .1, .9, [2, 20, 20, 2, 1])\n",
    "ax.set_title('The blue nodes are our PCA space, the yellow nodes our Latent space')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c289887",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "xscaler = StandardScaler()\n",
    "# xit = xscaler.inverse_transform\n",
    "X_reduced = xscaler.fit_transform(X_reduced)\n",
    "\n",
    "# Split into train, validation & test data\n",
    "X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(X_reduced, damage_classes, indices, train_size=0.7)\n",
    "X_val, X_test, y_val, y_test, ind_val, ind_test = train_test_split(X_test, y_test, ind_test, train_size=0.5)\n",
    "\n",
    "# Set up NN\n",
    "NN = manualMLPClassifier(solver='adam', hidden_layer_sizes=(20, 20, 2), activation='relu', random_state=1)\n",
    "\n",
    "# Initialize arrays to store data while training\n",
    "epochs = 3000\n",
    "latent_states_train = np.zeros((epochs, len(X_train), 2))\n",
    "latent_states_val = np.zeros((epochs, len(X_val), 2))\n",
    "\n",
    "# Grid PCA space\n",
    "dx_points = 20\n",
    "x = np.linspace(-4, 4, dx_points)\n",
    "y = np.linspace(-4, 4, dx_points)\n",
    "xv, yv = np.meshgrid(x, y, indexing='xy')\n",
    "xv_l = xv.reshape(-1, 1)\n",
    "yv_l = yv.reshape(-1, 1)\n",
    "PCA_points = np.append(xv_l, yv_l, axis=1)\n",
    "\n",
    "PCA_state_out = np.zeros((epochs, dx_points*dx_points))  # Store outcomes\n",
    "\n",
    "# Grid latent space\n",
    "grid_dx_points = 40\n",
    "x = np.linspace(0, 1, grid_dx_points)\n",
    "y = np.linspace(0, 1, grid_dx_points)\n",
    "xvl, yvl = np.meshgrid(x, y, indexing='xy')\n",
    "xvl_l = xvl.reshape(-1, 1)\n",
    "yvl_l = yvl.reshape(-1, 1)\n",
    "latent_grid_points = np.append(xvl_l, yvl_l, axis=1)\n",
    "\n",
    "latent_state_out = np.empty((epochs, grid_dx_points*grid_dx_points))          # Store latent outcomes\n",
    "latent_grid_deformed = np.empty((epochs, grid_dx_points * grid_dx_points, 2))  # Store latent positions\n",
    "\n",
    "# train NN\n",
    "NN, training_score, validation_score = NN_train_visualize(NN, X_train, y_train, X_val, y_val, classes=unique_classes, epochs=epochs, latent_states_train=latent_states_train, latent_states_val=latent_states_val, latent_state_out=latent_state_out, latent_grid_points=latent_grid_points, PCA_state_out=PCA_state_out, PCA_grid_points=PCA_points, transformed_latent_grid=latent_grid_deformed)\n",
    "\n",
    "# Compute train and test accuracy\n",
    "print(f\"Train accuracy: {NN.score(X_train, y_train)}\")\n",
    "print(f\"Validation accuracy: {NN.score(X_val, y_val)}\")\n",
    "print(f\"Test accuracy: {NN.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737acee2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.plot(training_score, label='Traininig score')\n",
    "plt.plot(validation_score, label='Validation score')\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylim(ymax=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score (Larger is better, maximum = 1)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf051f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we will interactively plot the complete training procedure of how the network makes its predictions. We show both the original PCA space, as well as the changing latent space (the final NN hidden layer). Note that after the final hidden layer, the output prediction is linear, as no more activation functions are used. This shows how this network creates a nonlinear mapping to a latent space in which a linear decision boundary can be drawn that defines a non-linear decision boundary in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f971609",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cur_sel_epoch = 0\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10,4))\n",
    "\n",
    "# reshape deformed latent grid to correct format\n",
    "latent_grid_deformed = latent_grid_deformed.reshape(epochs, grid_dx_points, grid_dx_points, 2)\n",
    "\n",
    "# Background Contour plot\n",
    "axs[0].contourf(xv, yv, PCA_state_out[cur_sel_epoch].reshape(xv.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "axs[1].contourf(latent_grid_deformed[cur_sel_epoch,:,:,0], latent_grid_deformed[cur_sel_epoch,:,:,1], latent_state_out[cur_sel_epoch].reshape(xvl.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "axs[2].contourf(latent_grid_deformed[cur_sel_epoch,:,:,0], latent_grid_deformed[cur_sel_epoch,:,:,1], latent_state_out[cur_sel_epoch].reshape(xvl.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "\n",
    "# Foreground datapoints\n",
    "# prediction_color = cmap(tmp_out_forward/2)\n",
    "plot_dynamic1 = axs[0].scatter(X_reduced[:,0], X_reduced[:,1], c=damage_classes, cmap=cmap, vmin=-0.5, vmax=1.5, s=40) # PCA\n",
    "plot_dynamic2 = axs[1].scatter(latent_states_train[cur_sel_epoch,:,0], latent_states_train[cur_sel_epoch,:,1], c=y_train, cmap=cmap, vmin=-0.5, vmax=1.5, s=40, linewidth=1) # Hidden: Train\n",
    "plot_dynamic3 = axs[2].scatter(latent_states_val[cur_sel_epoch,:,0], latent_states_val[cur_sel_epoch,:,1], c=y_val, cmap=cmap, vmin=-0.5, vmax=1.5, s=40, linewidth=1) # Hidden: Validate\n",
    "axs[0].set_title('Normalized PCA space')\n",
    "axs[1].set_title('Hidden space: Training')\n",
    "axs[2].set_title('Hidden space: Validation')\n",
    "\n",
    "# Add slider\n",
    "axepochs = plt.axes([0.2, -0.03, 0.6, 0.1])\n",
    "epoch_slider = Slider(ax=axepochs, label='Selected epoch', valmin=0, valmax=epochs)\n",
    "\n",
    "def update(val):\n",
    "    val = int(val)\n",
    "\n",
    "    # Re-draw Plot 0\n",
    "    axs[0].clear()\n",
    "    axs[0].contourf(xv, yv, PCA_state_out[val].reshape(xv.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "    axs[0].scatter(X_reduced[:,0], X_reduced[:,1], c=damage_classes, cmap=cmap, vmin=-0.5, vmax=1.5, s=40) # PCA\n",
    "    axs[0].set_title('Normalized PCA space')\n",
    "\n",
    "    # Re-draw Plot 1\n",
    "    axs[1].clear()\n",
    "    axs[1].contourf(latent_grid_deformed[val,:,:,0], latent_grid_deformed[val,:,:,1], latent_state_out[val].reshape(xvl.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "    axs[1].scatter(latent_states_train[val,:,0], latent_states_train[val,:,1], c=y_train, cmap=cmap, vmin=-0.5, vmax=1.5, s=40, linewidth=1) # Hidden: Train\n",
    "    axs[1].set_title('Latent space: Training samples')\n",
    "\n",
    "    # Re-draw Plot 2\n",
    "    axs[2].clear()\n",
    "    axs[2].contourf(latent_grid_deformed[val,:,:,0], latent_grid_deformed[val,:,:,1], latent_state_out[val].reshape(xvl.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "    axs[2].scatter(latent_states_val[val,:,0], latent_states_val[val,:,1], c=y_val, cmap=cmap, vmin=-0.5, vmax=1.5, s=40, linewidth=1) # Hidden: Validate\n",
    "    axs[2].set_title('Latent space: Validation samples')\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "epoch_slider.on_changed(update)\n",
    "\n",
    "\n",
    "cax = plt.axes([0.87, -0.002, 0.01, 0.08])\n",
    "plt.colorbar(plot_dynamic2, cax=cax, ticks=list(mapping.values()))\n",
    "cax.set_yticklabels(mapping.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b1fb1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "pred = NN.predict(X_test)\n",
    "prediction_color = cmap(pred/2)\n",
    "\n",
    "ax.contourf(xv, yv, PCA_state_out[-1].reshape(xv.shape), [0, 0.5, 1], cmap=cmap, alpha=0.3)\n",
    "\n",
    "plot1 = ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolors=prediction_color, cmap=cmap, vmin=-0.5, vmax=1.5, s=40, linewidth=2)\n",
    "cax = plt.colorbar(plot1, ticks=list(mapping.values()))\n",
    "ax.set_title('Test set predictions')\n",
    "cax.ax.set_yticklabels(mapping.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53d254",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Exercise 2:** use the slider above to understand what is happening inside the classification network. What happens when you for example select a different activation function?\n",
    "\n",
    "\n",
    "Now that we have seen how PCA and the classifier work, let us use both to do some analysis. **Excercise 3:** figure out if only using the second and third PCA mode leads to a more accurate prediction than only using the first and second mode, using the same network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1012bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Your code here\n",
    "# ------------------\n",
    "num_modes = 3\n",
    "pca = PCA(n_components=num_modes)\n",
    "pca.fit(X)\n",
    "print( f\"The variance explained by each component = {pca.explained_variance_ratio_}\")\n",
    "print( f\"The singular values = {pca.singular_values_}\")\n",
    "X_reduced = pca.transform(X)\n",
    "X_reduced = X_reduced[:,1:]     # Only select 2nd and 3rd eigenvalue\n",
    "\n",
    "# Normalize\n",
    "xscaler = StandardScaler()\n",
    "X_reduced = xscaler.fit_transform(X_reduced)\n",
    "\n",
    "# Split into train, validation & test data\n",
    "X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(X_reduced, damage_classes, indices, train_size=0.7)\n",
    "X_val, X_test, y_val, y_test, ind_val, ind_test = train_test_split(X_test, y_test, ind_test, train_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2d221",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NN = MLPClassifier(solver='adam', hidden_layer_sizes=(20, 20, 2), activation='relu', random_state=1, alpha=0.35)\n",
    "NN = manualMLPClassifier(solver='adam', hidden_layer_sizes=(20, 20, 2), activation='relu', random_state=1)\n",
    "NN, training_score, validation_score = NN_train(NN, X_train, y_train, X_val, y_val, epochs=1000)\n",
    "\n",
    "# Print final scores\n",
    "print(f\"Final train accuracy: {NN.score(X_train, y_train)}\")\n",
    "print(f\"Final validation accuracy: {NN.score(X_val, y_val)}\")\n",
    "print(f\"Final test accuracy: {NN.score(X_test, y_test)}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.plot(training_score, label='Traininig score')\n",
    "plt.plot(validation_score, label='Validation score')\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylim(ymax=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score (Larger is better, maximum = 1)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b229824",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Excercise 4:** use any number of features to try to train the most accurate classification model. Reflect about the implications of choosing more or less features on model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe7dd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Your code here\n",
    "# ------------------\n",
    "\n",
    "# -------------------\n",
    "num_modes = 25  #<- Change this value to include more or less modes\n",
    "# -------------------\n",
    "\n",
    "pca = PCA(n_components=num_modes)\n",
    "pca.fit(X)\n",
    "print( f\"The variance explained by each component = {pca.explained_variance_ratio_}\")\n",
    "print( f\"The singular values = {pca.singular_values_}\")\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r', 2)\n",
    "\n",
    "ax.scatter(X_reduced[:,1], X_reduced[:,2], X_reduced[:,0], c=damage_classes, cmap=cmap, vmin=-0.5, vmax=1.5, s=40)\n",
    "\n",
    "cax = plt.axes([0.8, .05, 0.01, 0.12])\n",
    "plt.colorbar(plot1, cax=cax, ticks=list(mapping.values()))\n",
    "cax.set_yticklabels(mapping.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c512a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "xscaler = StandardScaler()\n",
    "X_reduced = xscaler.fit_transform(X_reduced)\n",
    "\n",
    "# Split into train, validation & test data\n",
    "X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(X_reduced, damage_classes, indices, train_size=0.7)\n",
    "X_val, X_test, y_val, y_test, ind_val, ind_test = train_test_split(X_test, y_test, ind_test, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff09b30",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NN = MLPClassifier(solver='adam', hidden_layer_sizes=(40, 40), activation='relu', random_state=1, alpha=2)\n",
    "NN, training_score, validation_score = NN_train(NN, X_train, y_train, X_val, y_val, epochs=1000)\n",
    "\n",
    "# Print final scores\n",
    "print(f\"Final train accuracy: {NN.score(X_train, y_train)}\")\n",
    "print(f\"Final validation accuracy: {NN.score(X_val, y_val)}\")\n",
    "print(f\"Final test accuracy: {NN.score(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba13e3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = NN.predict(X_test)\n",
    "\n",
    "# Plot score and test prediction\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(training_score, label='Traininig')\n",
    "ax.plot(validation_score, label='Validation')\n",
    "ax.set_title('Score during training')\n",
    "ax.set_xlim(xmin=0)\n",
    "ax.set_ylim(ymax=1.02)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Score (Larger is better, maximum = 1)')\n",
    "ax.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2,projection='3d')\n",
    "prediction_color = cmap(pred/2)\n",
    "plot1 = ax2.scatter(X_test[:,1], X_test[:,2], X_test[:,3], c=y_test, cmap=cmap, edgecolors=prediction_color, vmin=-0.5, vmax=1.5, s=60, linewidth=3)\n",
    "ax2.set_title('Test set projected in 3D')\n",
    "\n",
    "cax = plt.axes([0.84, 0.05, 0.01, 0.12])\n",
    "plt.colorbar(plot1, cax=cax, ticks=list(mapping.values()))\n",
    "cax.set_yticklabels(mapping.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3feaf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare this result to the accuracy you obtained using 2 sensors. You will likely observe that using all data with dimensionality reduction provided a better score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6426d0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "In this notebook, we looked at predicting classes instead of continuous values. First, looking at model selection for classification, the aim was to find the best model using the data from two specific sensors. Following this, we looked at what happens inside a classification neural network. Finally, by selecting using several modes of PCA, the highest classification score was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649ac11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b3301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
